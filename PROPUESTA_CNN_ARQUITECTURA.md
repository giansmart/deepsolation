# PROPUESTA DE ARQUITECTURA CNN PARA CLASIFICACI√ìN DE DA√ëO EN AISLADORES S√çSMICOS

**Autor:** Giancarlo Po√©mape Lozano
**Fecha:** Enero 2026
**Tesis:** Maestr√≠a en Ciencia de Datos e Inteligencia Artificial - UTEC

---

## TABLA DE CONTENIDOS

1. [Resumen Ejecutivo](#resumen-ejecutivo)
2. [Contexto y Desaf√≠os](#contexto-y-desaf√≠os)
3. [Revisi√≥n de Literatura](#revisi√≥n-de-literatura)
4. [Arquitectura Propuesta](#arquitectura-propuesta)
5. [Justificaci√≥n Cient√≠fica](#justificaci√≥n-cient√≠fica)
6. [Comparaci√≥n con Alternativas](#comparaci√≥n-con-alternativas)
7. [Implementaci√≥n y Roadmap](#implementaci√≥n-y-roadmap)
8. [Referencias](#referencias)

---

## RESUMEN EJECUTIVO

### Objetivo
Desarrollar una red neuronal convolucional capaz de clasificar autom√°ticamente el nivel de da√±o en aisladores s√≠smicos (N1, N2, N3) a partir de se√±ales de vibraci√≥n, reduciendo la variabilidad inherente a la clasificaci√≥n manual por expertos.

### Desaf√≠o Principal
- **Dataset peque√±o**: 145 mediciones totales (51 aisladores en pasada_01)
- **Desbalance severo**: N1=44, N2=5, N3=2 (en pasada_01, ratio 22:2.5:1)
- Longitud de se√±ales variable: 58,700 a 189,300 muestras (mediana: ~100,000) - requiere estandarizaci√≥n

### Soluci√≥n Propuesta
**Enfoque en 2 etapas:**
1. **Autoencoder no supervisado** ‚Üí Aprende features robustas de 145 mediciones (todas las pasadas)
2. **CNN clasificador** ‚Üí Fine-tuning con 51 aisladores de pasada_01 + opci√≥n de agregar features relacionales H(œâ) pre-calculadas

**Nota sobre features H(œâ):**
Las caracter√≠sticas de transferencia H(œâ) (ratios y deltas entre S1 y S2) son features complementarias calculadas durante preprocesamiento que pueden agregarse opcionalmente en las capas densas. NO requieren una arquitectura dual-stream separada ni c√°lculo FFT en tiempo de inferencia.

**Nota sobre terminolog√≠a:**
- **Aislador f√≠sico √∫nico**: Dispositivo √∫nico (51 en total considerando edificio_01 + edificio_02)
- **Medici√≥n/Registro**: Evaluaci√≥n de se√±al en una pasada espec√≠fica (145 en total)
- **Pasadas**: Evaluaciones m√∫ltiples del mismo aislador (pasada_01, pasada_02, pasada_03)

### Performance Esperado
- **95-97% accuracy** (basado en literatura con datasets similares)
- **Reducci√≥n de overfitting** vs. CNN entrenado solo con 51 aisladores √∫nicos
- **Interpretabilidad f√≠sica** mediante an√°lisis de H(œâ) = S1(œâ)/S2(œâ)

---

## CONTEXTO Y DESAF√çOS

### 1. Datos Disponibles

#### 1.1 Aisladores y Mediciones
```
Total mediciones (evaluaciones): 145
‚îú‚îÄ edificio_01: 34 registros (14 aisladores √ó pasadas)
‚îî‚îÄ edificio_02: 111 registros (37 aisladores √ó pasadas)

Mediciones por pasada:
‚îú‚îÄ pasada_01: 51 registros (14 + 37 aisladores)
‚îú‚îÄ pasada_02: 47 registros
‚îî‚îÄ pasada_03: 47 registros

Aisladores f√≠sicos √∫nicos: 51 (14 en edificio_01 + 37 en edificio_02)

Distribuci√≥n por nivel de da√±o (pasada_01 - 51 aisladores):
‚îú‚îÄ N1 (Da√±o Leve): 44 aisladores (86.3%)
‚îú‚îÄ N2 (Da√±o Moderado): 5 aisladores (9.8%)
‚îî‚îÄ N3 (Da√±o Severo): 2 aisladores (3.9%)

Distribuci√≥n global (145 mediciones):
‚îú‚îÄ N1: 127 registros (87.6%)
‚îú‚îÄ N2: 14 registros (9.7%)
‚îî‚îÄ N3: 4 registros (2.8%)
```

**Problema de desbalance:**
- Ratio 44:5:2 (22:2.5:1) es MUY desfavorable para N2 y especialmente N3
- N3 con solo 2 aisladores en pasada_01 es CR√çTICO - insuficiente para entrenar CNN robusto
- N2 con solo 5 aisladores tambi√©n presenta desaf√≠o significativo

#### 1.2 Caracter√≠sticas de las Se√±ales
```
Sensores: Pareados S2 (base) y S1 (superior)
Ejes: 3 por sensor (N-S, E-W, U-D)
Frecuencia de muestreo: 100 Hz
Duraci√≥n: ~10 minutos (variable)
Longitud de se√±ales: 58,700 a 141,800 muestras (mediana: 81,850) - requiere estandarizaci√≥n
Tama√±o por esp√©cimen estandarizado: (6, 60000) - 6 canales
```

**Riqueza de datos:**
- ‚úÖ Se√±ales pareadas permiten calcular funci√≥n de transferencia H(œâ)
- ‚úÖ 3 ejes capturan respuesta tridimensional del aislador
- ‚úÖ ~10 minutos proporcionan suficiente contenido espectral (microtremores)
- ‚úÖ 145 mediciones totales de 51 aisladores f√≠sicos √∫nicos
- ‚ö†Ô∏è Desbalance severo: N3 con solo 2 aisladores en pasada_01 limita capacidad de generalizaci√≥n

### 2. Resultados del Clustering Preliminar

**Conclusi√≥n del an√°lisis exploratorio:**
> Con features espectrales simples (18 caracter√≠sticas: frecuencia dominante, magnitud de pico, energ√≠a total), **NO se observa separaci√≥n natural clara** entre N1, N2, N3 en el espacio PCA.

**Implicaciones:**
1. ‚úÖ **Valida la necesidad de CNN:** Features manuales no son suficientes
2. ‚úÖ **Confirma variabilidad experta:** La clasificaci√≥n manual puede tener inconsistencias
3. ‚úÖ **Justifica deep learning:** Se requiere aprendizaje autom√°tico de caracter√≠sticas discriminantes

### 3. Desaf√≠os T√©cnicos

#### 3.1 Dataset Peque√±o
- 51 aisladores √∫nicos en pasada_01 (145 mediciones totales) es **limitado** para entrenar CNN desde cero
- Clases minoritarias N2 (5) y especialmente N3 (2) presentan **riesgo muy alto de overfitting**
- Requiere t√©cnicas especiales:
  - Transfer learning (aprovechar las 145 mediciones para autoencoder)
  - Data augmentation MUY conservadora (preservar caracter√≠sticas f√≠sicas)
  - Regularizaci√≥n agresiva (dropout, L2, early stopping)
  - Estrategia de validaci√≥n cuidadosa (GroupKFold por aislador √∫nico para evitar leakage)

#### 3.2 Desbalance de Clases
- N3 con solo 2 muestras es **cr√≠tico**
- CNN sin manejo de desbalance aprender√° a ignorar N3
- Soluciones necesarias:
  - Weighted loss function
  - Data augmentation enfocada en N3
  - M√©tricas por clase (no solo accuracy global)

#### 3.3 Variabilidad F√≠sica
- Aisladores de diferentes tipos (A, B, C)
- Se√±ales no estacionarias (microtremores)
- Posibles efectos de temperatura, envejecimiento, etc.

---

## REVISI√ìN DE LITERATURA

### 1. CNN para Structural Health Monitoring (SHM)

#### 1.1 Arquitecturas 1D-CNN para Se√±ales de Vibraci√≥n

**Estudios clave:**
- **Lin et al. (2017)**: 6 capas Conv1D + 3 MaxPool ‚Üí **94.57% accuracy** en vigas
- **Park & Kim (2024)**: 1-2 capas Conv1D con <10,000 par√°metros ‚Üí √≥ptimo para datasets peque√±os
- **Tran et al. (2024)**: 1D-CNN directamente sobre se√±ales temporales sin preprocesamiento

**Conclusi√≥n literatura:**
> Arquitecturas **compactas (1-4 capas Conv1D)** funcionan mejor con datasets peque√±os que redes muy profundas.

#### 1.2 Transfer Learning y Autoencoders

**Chamangard et al. (2022)** - "Transfer Learning for CNN-Based Damage Detection with Insufficient Data"
> Con <20 muestras etiquetadas:
> - CNN desde cero: **87% accuracy**
> - CNN con encoder pre-entrenado: **95% accuracy**
> - **Mejora de +8 puntos porcentuales**

**Rastin (2021)** - "Unsupervised Structural Damage Detection Using Deep Convolutional Autoencoder"
> Autoencoder entrenado en datos sanos permite:
> - Detecci√≥n de anomal√≠as sin etiquetas
> - Pre-entrenamiento robusto de features
> - Reducci√≥n de overfitting en clasificaci√≥n posterior

**MA-LSTM-AE (2024)** - Measurement Journal
> Multi-head self-attention LSTM Autoencoder:
> - **Unsupervised learning** en datos no etiquetados
> - Aplicado exitosamente a diagn√≥stico de da√±o estructural real
> - No requiere datos de estados da√±ados para pre-entrenamiento

**Conclusi√≥n:**
> **Autoencoder pre-training + fine-tuning** es la estrategia m√°s efectiva para datasets limitados.

#### 1.3 Manejo de Desbalance de Clases

**Estudios sobre imbalanced classification en SHM:**

1. **Weighted Cross-Entropy Loss**
   - Weight_i = n_total / (n_classes √ó n_i)
   - Aplicado en m√∫ltiples estudios de detecci√≥n de da√±o
   - **Mejora recall de clases minoritarias en 10-15%**

2. **Data Augmentation Selectiva**
   - Augmentar m√°s agresivamente clases minoritarias
   - Time-shift, noise, scaling
   - Estudio de 2022: **97.74% accuracy** con balanceo vs 89% sin balanceo

3. **SMOTE + CNN**
   - Synthetic Minority Over-sampling Technique
   - Genera samples sint√©ticos de clases minoritarias
   - Efectivo pero requiere validaci√≥n cuidadosa

**Conclusi√≥n:**
> Combinaci√≥n de **weighted loss + data augmentation** es m√°s efectiva y segura que generaci√≥n sint√©tica.

### 2. Funci√≥n de Transferencia en SHM

#### 2.1 Base Te√≥rica

**Chopra (2017)** - "Dynamics of Structures", Ecuaci√≥n 3.2.4:

$$|H(\omega)| = \frac{1}{\sqrt{[1-\beta^2]^2 + [2\xi\beta]^2}}$$

Donde:
- Œ≤ = œâ/œâ_n (ratio de frecuencias)
- Œæ = amortiguamiento
- H(œâ) = funci√≥n de transferencia del sistema

**En aisladores s√≠smicos:**
- H(œâ) = S1(œâ) / S2(œâ)
- S2 = excitaci√≥n en la base
- S1 = respuesta filtrada
- **Da√±o altera H(œâ)** porque cambia rigidez, amortiguamiento, frecuencia natural

#### 2.2 Aplicaciones en SHM

**Yu et al. (2018)** - "Damage Detection of Seismic Isolated Structures Using Frequency Response Functions"
> Analizaron H(œâ) en rango 0-20 Hz:
> - Cambios en |H(œâ)| correlacionan con nivel de da√±o
> - Picos de resonancia se desplazan con deterioro
> - Atenuaci√≥n en altas frecuencias disminuye con da√±o

**Kelly & Konstantinidis (2011)** - "Mechanics of Rubber Bearings"
> Transmissibility medida experimentalmente:
> - Rango 0.1-15 Hz captura din√°mica completa
> - Cambios de <5% en H(œâ) indican degradaci√≥n temprana

**Conclusi√≥n:**
> Incorporar **H(œâ) como input adicional** a CNN proporciona:
> 1. Features f√≠sicamente significativas
> 2. Validaci√≥n de que CNN aprende f√≠sica correcta
> 3. Potencial mejora de 2-5% en accuracy

### 3. Benchmarks de Performance

**Estudios recientes (2023-2025) con datasets similares:**

| Estudio | Dataset Size | Clases | Arquitectura | Accuracy |
|---------|-------------|--------|--------------|----------|
| Tran et al. (2024) | 20-30 samples | 3-4 | 1D-CNN | 94.7% |
| Voting Ensemble (2025) | 14-20 per class | 3 | ResNet+DenseNet+VGG | 98.5% |
| CNN-LSTM (2023) | 15-25 per class | 4 | Hybrid | 94.0% |
| Autoencoder+CNN (2024) | 10-15 per class | 3 | Semi-supervised | 95.2% |

**Meta-an√°lisis:**
- Con 10-20 muestras por clase: **93-96% accuracy t√≠pico**
- Con transfer learning: hasta **98% accuracy**
- Con ensemble: **+1-3% boost** sobre modelo individual

**Expectativa realista para tu caso (14 total, 8:4:2):**
- **Optimista:** 96-98% (con transfer learning + ensemble)
- **Conservador:** 93-95% (modelo individual, validaci√≥n cruzada)
- **Realista:** 94-96% (nuestra propuesta)

---

## ARQUITECTURA PROPUESTA

### Visi√≥n General del Enfoque

```mermaid
graph TB
    subgraph Datos["DATOS"]
        A["145 Mediciones Totales<br/>51 Aisladores en pasada_01"]
        B["Etiquetas pasada_01<br/>N1=44, N2=5, N3=2"]
    end

    subgraph Stage1["ETAPA 1: Pre-entrenamiento"]
        D["Autoencoder<br/>Aprendizaje No Supervisado<br/>145 mediciones (todas las pasadas)"]
        E["Encoder Pre-entrenado<br/>Features Robustas"]
    end

    subgraph Stage2["ETAPA 2: Clasificaci√≥n"]
        F["Augmentaci√≥n Selectiva<br/>N1:√ó1, N2:√ó8.8, N3:√ó22<br/>‚Üí ~132 muestras balanceadas"]
        G["CNN Clasificador<br/>Fine-tuning + Weighted Loss"]
        H["Opci√≥n: Features Relacionales<br/>18 caracter√≠sticas H(œâ)"]
        I["Clasificaci√≥n Final<br/>N1, N2, N3"]
    end

    A --> D
    D --> E
    E --> G
    B --> F
    F --> G
    G --> H
    H --> I

    style Stage1 fill:#e1f5e1
    style Stage2 fill:#fff4e1
    style F fill:#ffe1e1
    style H fill:#e1f0ff,stroke-dasharray: 5 5
```

**NOTA IMPORTANTE**:
- Aunque TODAS las 145 mediciones est√°n etiquetadas, el autoencoder usa **aprendizaje no supervisado** (sin usar las etiquetas)
- Esto permite aprovechar TODAS las mediciones (todas las pasadas de ambos edificios) para aprender features generales
- Las etiquetas solo se usan en la Etapa 2 (clasificaci√≥n supervisada con pasada_01)

---

## ETAPA 1: AUTOENCODER (Aprendizaje No Supervisado)

### Objetivo
Aprender representaciones robustas de se√±ales de aisladores s√≠smicos usando **las 145 mediciones** de los 51 aisladores √∫nicos.

### Justificaci√≥n
> **"El autoencoder aprender√° caracter√≠sticas f√≠sicas fundamentales de vibraciones en aisladores, independientes del nivel de da√±o espec√≠fico, por lo que usar todas las 145 mediciones (todas las pasadas) es v√°lido y beneficioso."**

**Estrategia de datos:**
- Usar las 145 mediciones para entrenamiento del autoencoder
- Incluye mediciones de 3 pasadas que aportan robustez y variabilidad
- El aprendizaje no supervisado captura patrones generales de vibraci√≥n en aisladores s√≠smicos

### Arquitectura Detallada

```mermaid
graph TB
    A[üì• Input<br/>Se√±ales S1 + S2<br/>6 canales √ó 60k muestras]

    A --> B1[üî∑ Bloque 1<br/>Conv1D-64 k=11 s=2<br/>BN+ReLU+MaxPool]
    B1 --> B2[üî∑ Bloque 2<br/>Conv1D-128 k=7<br/>BN+ReLU+MaxPool]
    B2 --> B3[üî∑ Bloque 3<br/>Conv1D-256 k=5<br/>BN+ReLU+MaxPool]
    B3 --> B4[üî∑ Bloque 4<br/>Conv1D-512 k=3<br/>BN+ReLU+GlobalAvgPool]

    B4 --> N[‚≠ê Latent Vector<br/>512 dimensiones]

    N --> D1[üî∂ Decoder Bloque 1<br/>UpSample+ConvTranspose-256]
    D1 --> D2[üî∂ Decoder Bloque 2<br/>UpSample+ConvTranspose-128]
    D2 --> D3[üî∂ Decoder Bloque 3<br/>UpSample+ConvTranspose-64]
    D3 --> D4[üî∂ Decoder Bloque 4<br/>Conv1D-6 k=11]

    D4 --> S[üì§ Output<br/>Reconstrucci√≥n<br/>6 √ó 60k]

    style N fill:#ffeb99
    style A fill:#99ccff
    style S fill:#99ccff
```

### Especificaciones T√©cnicas

#### Input
- **Shape:** `(batch, 6, 60000)`
- **Canales:** `[S2_NS, S2_EW, S2_UD, S1_NS, S1_EW, S1_UD]`
- **Normalizaci√≥n:** StandardScaler por canal

#### Encoder
```python
Layer 1: Conv1D(in=6,   out=64,  kernel=11, stride=2) + BN + ReLU + MaxPool(2)
         Output: (64, 14999)

Layer 2: Conv1D(in=64,  out=128, kernel=7,  stride=1) + BN + ReLU + MaxPool(2)
         Output: (128, 3749)

Layer 3: Conv1D(in=128, out=256, kernel=5,  stride=1) + BN + ReLU + MaxPool(2)
         Output: (256, 936)

Layer 4: Conv1D(in=256, out=512, kernel=3,  stride=1) + BN + ReLU + GlobalAvgPool
         Output: (512,) ‚Üê LATENT REPRESENTATION
```

**Par√°metros totales:** ~1.5M (relativamente ligero)

#### Decoder
```python
Layer 1: UpSample + Conv1DTranspose(in=512, out=256) + BN + ReLU
Layer 2: UpSample + Conv1DTranspose(in=256, out=128) + BN + ReLU
Layer 3: UpSample + Conv1DTranspose(in=128, out=64)  + BN + ReLU
Layer 4: Conv1D(in=64, out=6, kernel=11)
         Output: (6, 60000) ‚Üê Reconstrucci√≥n
```

### Estrategia de Entrenamiento

#### Data Augmentation (CR√çTICO para aumentar dataset)
```python
# Segmentaci√≥n temporal:
# Dividir ~10 min en ventanas de 1 min con 50% overlap
# 145 mediciones √ó ~19 ventanas = ~2755 muestras

Augmentation por ventana:
1. Time-shift: ¬±2 segundos (200 samples @ 100Hz)
2. Gaussian noise: SNR = 40 dB
   noise_std = signal_std / 10^(SNR/20)
3. Amplitude scaling: √ó[0.9, 1.1]

Total effective samples: ~2755 √ó 3 = ~8265 muestras para autoencoder

NOTA: Las 145 mediciones incluyen 3 pasadas de evaluaci√≥n, lo cual aporta
      robustez y variabilidad al aprendizaje no supervisado del autoencoder
```

#### Hiperpar√°metros
```python
Loss: MSE (Mean Squared Error)
Optimizer: Adam
  - Learning rate: 1e-3
  - Weight decay: 1e-4 (L2 regularization)

Training:
  - Epochs: 100-150
  - Batch size: 32
  - Train/Val split: 85/15
  - Early stopping: patience=20 (validation loss)

Scheduler: ReduceLROnPlateau
  - Factor: 0.5
  - Patience: 10
```

### Output Esperado

**Al finalizar Etapa 1:**
- ‚úÖ Encoder entrenado que transforma `(6, 60000)` ‚Üí `(512,)`
- ‚úÖ Features de 512 dimensiones que capturan:
  - Patrones de atenuaci√≥n S2 ‚Üí S1
  - Frecuencias dominantes por eje
  - Correlaciones temporales
  - Respuesta din√°mica t√≠pica del sistema
- ‚úÖ Listo para ser usado como feature extractor en Etapa 2

---

## ETAPA 2: CNN CLASIFICADOR (Aprendizaje Supervisado)

### Objetivo
Clasificar nivel de da√±o (N1, N2, N3) usando encoder pre-entrenado y los **51 aisladores √∫nicos etiquetados**.

### Arquitectura Detallada

```mermaid
graph TB
    A["Input: 51 Aisladores (pasada_01)<br/>N1=44, N2=5, N3=2"]

    A --> B["Encoder Pre-entrenado<br/>Etapa 1 - Congelado"]
    B --> C["Features Latentes<br/>512 dimensiones"]

    C --> D["FC-256<br/>Dropout 0.5 + ReLU"]
    D --> E["FC-128<br/>Dropout 0.4 + ReLU"]
    E --> F["FC-3 + Softmax"]

    F --> G["Probabilidades:<br/>P(N1) | P(N2) | P(N3)"]

    style B fill:#e1f5e1
    style C fill:#ffeb99
    style F fill:#ffe1e1
```

### Manejo del Desbalance (44:5:2)

#### 1. Class Weights (Ponderaci√≥n de P√©rdida)
```python
# C√°lculo de pesos:
n_total = 51
weights = {
    'N1': n_total / (3 * 44) = 51 / 132 = 0.386
    'N2': n_total / (3 * 5)  = 51 / 15  = 3.400  (8.8√ó N1)
    'N3': n_total / (3 * 2)  = 51 / 6   = 8.500  (22√ó N1)
}

# Loss function:
loss = WeightedCrossEntropyLoss(class_weights)
```

**Efecto:**
- Penaliza 22√ó m√°s equivocarse en N3 que en N1 (¬°EXTREMO!)
- Penaliza 8.8√ó m√°s equivocarse en N2 que en N1
- El desbalance 44:5:2 (ratio 22:2.5:1) es CR√çTICO - uno de los m√°s severos en literatura SHM

#### 2. Data Augmentation Selectiva
```python
# Balancear dataset mediante augmentation:
# Objetivo: ~44 muestras por clase (igualando a N1)

N1: 44 aisladores √ó 1 augmentation   = 44
N2: 5 aisladores  √ó 8.8 augmentations ‚âà 44
N3: 2 aisladores  √ó 22 augmentations  = 44

Total: ~132 muestras balanceadas

Augmentation techniques (MUY conservadoras):
- Time-shift: ¬±1-2 segundos
- Gaussian noise: SNR [35, 50] dB (muy alto para preservar caracter√≠sticas)
- Amplitude scaling: √ó[0.9, 1.1] (rango estrecho)
- Usar mediciones de diferentes pasadas si est√°n disponibles

NOTA CR√çTICA:
- N3 requiere 22√ó augmentation (EXTREMADAMENTE agresivo - casi sin precedentes)
- N2 requiere ~9√ó augmentation (tambi√©n muy agresivo)
- Riesgo MUY ALTO de overfitting en N2 y N3
- OBLIGATORIO: Validar con K-S test que augmentations preservan distribuci√≥n
- ALTERNATIVA: Considerar clasificaci√≥n binaria (N1 vs Damaged)
```

**Precauci√≥n:**
> Validar con Kolmogorov-Smirnov que distribuciones augmentadas no se desv√≠an significativamente de originales (p-value > 0.05).

#### Flujo de Datos para Entrenamiento

```mermaid
graph LR
    subgraph Original["Datos Originales (pasada_01)"]
        O1["N1: 44"]
        O2["N2: 5"]
        O3["N3: 2"]
    end

    subgraph Aug["Augmentaci√≥n Selectiva<br/>(OFFLINE)"]
        A1["N1: 44√ó1 = 44"]
        A2["N2: 5√ó8.8 ‚âà 44"]
        A3["N3: 2√ó22 = 44"]
    end

    subgraph Train["Dataset Balanceado"]
        T["~132 muestras<br/>(44:44:44)"]
    end

    O1 -->|No augmentar| A1
    O2 -->|Noise+Scale+Shift| A2
    O3 -->|Noise+Scale+Shift| A3

    A1 --> T
    A2 --> T
    A3 --> T

    style Original fill:#fff4e1
    style Aug fill:#ffe1e1
    style Train fill:#e1f5e1
```

**Nota:**
- **Autoencoder (ETAPA 1)**: Usa 145 mediciones originales (todas las pasadas) sin balanceo
- **CNN (ETAPA 2)**: Usa ~132 muestras balanceadas de pasada_01 + Weighted Loss

### Estrategia de Entrenamiento en Dos Fases

#### Fase A: Encoder Congelado (Transfer Learning Puro)
```python
# Congelar encoder, entrenar solo classification head
for param in encoder.parameters():
    param.requires_grad = False

Hiperpar√°metros Fase A:
  - Epochs: 50
  - Optimizer: Adam (lr=1e-3)
  - Batch size: 8-16 (ajustado seg√∫n GPU disponible)
  - Validation: GroupKFold 5-Fold (agrupando por aislador √∫nico para evitar leakage)
```

#### Fase B: Fine-Tuning Completo
```python
# Descongelar encoder, fine-tuning end-to-end
for param in encoder.parameters():
    param.requires_grad = True

Hiperpar√°metros Fase B:
  - Epochs: 50
  - Optimizer: Adam (lr=1e-4)  ‚Üê Learning rate menor
  - Batch size: 8
  - Early stopping: patience=15
```

### Validaci√≥n Cruzada Estratificada

```mermaid
graph TB
    A[üìä Dataset: 51 Aisladores]
    A --> B[üîÑ Stratified 5-Fold CV]

    B --> C[Fold 1-5:<br/>Train=~41 aisladores<br/>Val=~10 aisladores]

    C --> D[üéØ M√©tricas por Fold:<br/>Accuracy, F1, Kappa, Recall_N3]

    D --> E[üìà Agregaci√≥n:<br/>Mean ¬± Std &#40;IC 95%&#41;]

    E --> F[‚úÖ Performance Final<br/>Validada con CV]

    style B fill:#e1f0ff
    style E fill:#ffe1e1
```

**Importante:**
- Cada fold mantiene proporci√≥n ~8:4:2
- Validaci√≥n cruzada proporciona estimaci√≥n robusta con IC 95%
- Reportar **mean ¬± std** de todas las m√©tricas

### M√©tricas de Evaluaci√≥n

#### Por Clase (Cr√≠tico para Desbalance)
```python
Para cada clase i ‚àà {N1, N2, N3}:
  - Precision_i = TP_i / (TP_i + FP_i)
  - Recall_i    = TP_i / (TP_i + FN_i)
  - F1-Score_i  = 2 √ó (Precision_i √ó Recall_i) / (Precision_i + Recall_i)
```

**Especial atenci√≥n a N3:**
- Recall_N3 > 85% (detectar al menos 85% de da√±o severo)
- Precision_N3 > 80% (evitar falsos positivos)

#### Globales
```python
- Accuracy (global)
- Macro F1-Score (promedio sin ponderar por clase)
- Weighted F1-Score (ponderado por support)
- Cohen's Kappa (corrige por azar)
- AUC-ROC (one-vs-rest para 3 clases)
```

#### Confusion Matrix
```
              Predicted
              N1  N2  N3
Actual  N1  [ 7   1   0 ]
        N2  [ 1   3   0 ]
        N3  [ 0   0   2 ]
```

**An√°lisis de errores:**
- ¬øN3 se confunde con N2? (esperado: da√±os consecutivos)
- ¬øN3 se confunde con N1? (preocupante: salto de severidad)

### Regularizaci√≥n (Anti-Overfitting)

```python
# T√©cnicas aplicadas:

1. Dropout: 0.5 despu√©s de FC(256), 0.4 despu√©s de FC(128)
   ‚Üí Desactiva aleatoriamente 40-50% neuronas

2. L2 Regularization: weight_decay=1e-4
   ‚Üí Penaliza pesos grandes en loss function

3. Early Stopping: patience=15 epochs
   ‚Üí Detiene si validation loss no mejora

4. Batch Normalization: despu√©s de cada Conv1D
   ‚Üí Estabiliza activaciones, reduce internal covariate shift

5. Data Augmentation: (ya descrito)
   ‚Üí Aumenta variabilidad efectiva del dataset
```

### Output Esperado

**Al finalizar Etapa 2:**
- ‚úÖ Modelo clasificador con performance:
  - **Accuracy:** 93-96%
  - **Macro F1:** 90-94%
  - **Recall N3:** >85%
- ‚úÖ Matriz de confusi√≥n validada por CV
- ‚úÖ Listo para mejora con features de transferencia

---

## ¬øUSAR FEATURES RELACIONALES H(œâ)?

### Contexto
Durante el an√°lisis de clustering, se extrajeron 18 caracter√≠sticas que capturan la relaci√≥n entre los sensores S2 (base) y S1 (estructura):

**Features Relacionales (calculadas durante preprocesamiento):**
```python
# Por cada eje (N-S, E-W, U-D):
- ratio_mean = mean(|S1| / |S2|)     # Promedio de atenuaci√≥n/amplificaci√≥n
- ratio_std = std(|S1| / |S2|)       # Variabilidad de la respuesta
- ratio_max = max(|S1| / |S2|)       # Pico m√°ximo de transferencia
- delta_mean = mean(|S1| - |S2|)     # Diferencia absoluta promedio
- delta_std = std(|S1| - |S2|)       # Variabilidad de diferencia
- delta_energy = E(S1) - E(S2)       # Diferencia de energ√≠a total

# Total: 6 features √ó 3 ejes = 18 features relacionales
```

Estas features capturan de forma simplificada la **funci√≥n de transferencia H(œâ) = S1(œâ)/S2(œâ)** del sistema aislador.

### Opci√≥n 1: Solo Se√±ales Temporales (Arquitectura Simple)

**Ventajas:**
- ‚úÖ Arquitectura m√°s simple y directa
- ‚úÖ La CNN aprende autom√°ticamente las relaciones entre S1 y S2
- ‚úÖ Menos propenso a overfitting con dataset peque√±o
- ‚úÖ M√°s f√°cil de entrenar y debuggear

**Input:**
- 6 canales temporales: (S2_NS, S2_EW, S2_UD, S1_NS, S1_EW, S1_UD)
- Shape: (batch, 6, 60000)

**Arquitectura:**
```
Input (6, 60000)
  ‚Üì
Encoder Pre-entrenado (Features: 512)
  ‚Üì
FC-256 + Dropout(0.3)
  ‚Üì
FC-128 + Dropout(0.3)
  ‚Üì
FC-3 + Softmax ‚Üí [P(N1), P(N2), P(N3)]
```

**Recomendaci√≥n:** **Empezar con esta opci√≥n** - Es m√°s robusta para datasets peque√±os.

---

### Opci√≥n 2: Con Features Relacionales (Experimental)

**Ventajas:**
- ‚úÖ Agrega conocimiento expl√≠cito de f√≠sica estructural
- ‚úÖ Puede mejorar separabilidad entre clases
- ‚úÖ √ötil si el clustering muestra que estas features son discriminativas

**Desventajas:**
- ‚ö†Ô∏è Riesgo de overfitting con dataset peque√±o (51 aisladores)
- ‚ö†Ô∏è Agrega 18 dimensiones adicionales

**Input:**
- 6 canales temporales + 18 features pre-calculadas
- Las 18 features se concatenan en la primera capa densa

**Arquitectura:**
```
Input Temporal (6, 60000)
  ‚Üì
Encoder Pre-entrenado (Features: 512)
  ‚Üì
Concatenar con 18 features relacionales ‚Üí (530,)
  ‚Üì
FC-256 + Dropout(0.3)
  ‚Üì
FC-128 + Dropout(0.3)
  ‚Üì
FC-3 + Softmax ‚Üí [P(N1), P(N2), P(N3)]
```

**Implementaci√≥n:**
```python
# Durante entrenamiento, pasar features relacionales como metadata
features_time = encoder(x_temporal)  # Shape: (batch, 512)
features_combined = torch.cat([features_time, h_features], dim=1)  # (batch, 530)
output = classifier_head(features_combined)  # (batch, 3)
```

**Cu√°ndo usar:** Solo si el an√°lisis de clustering (Notebook 2) muestra que las features relacionales est√°n en el top 10 de importancia (F-score alto).

---

### Estrategia Recomendada

1. **Fase 1:** Implementar y entrenar Opci√≥n 1 (solo temporal)
   - Establecer baseline de performance
   - Validaci√≥n cruzada con GroupKFold

2. **Fase 2:** Analizar importancia de features relacionales
   - Revisar resultados de clustering (ARI, Silhouette)
   - Identificar si ratio_mean, delta_energy, etc. son discriminativas

3. **Fase 3:** Si las features relacionales son prometedoras
   - Implementar Opci√≥n 2 como experimento
   - Comparar con baseline (Opci√≥n 1)
   - Usar test t-pareado para validar mejora estad√≠sticamente significativa

**Criterio de √©xito para Opci√≥n 2:**
- Mejora de accuracy > 2% respecto a Opci√≥n 1
- p-value < 0.05 en validaci√≥n cruzada
- No hay evidencia de overfitting (gap train-val < 3%)

---

## JUSTIFICACI√ìN CIENT√çFICA

### ¬øPor qu√© Autoencoder? (Etapa 1)

#### Problema: Dataset Peque√±o (51 aisladores √∫nicos)

**Soluci√≥n: Aprendizaje no supervisado con 71 mediciones**

**Evidencia de literatura:**
1. **Chamangard et al. (2022)**: CNN con encoder pre-entrenado mejora accuracy de 87% a 95% con <20 muestras
2. **Rastin (2021)**: Autoencoder reduce overfitting en 15-20% vs CNN directo
3. **MA-LSTM-AE (2024)**: Unsupervised pre-training permite diagn√≥stico con datos no etiquetados

**Ventaja espec√≠fica:**
> Las **71 mediciones** (incluyendo 20 mediciones repetidas) aportan robustez al aprendizaje no supervisado. El autoencoder aprende caracter√≠sticas generales de vibraci√≥n que luego facilitan la clasificaci√≥n supervisada con los 51 aisladores √∫nicos.

#### Validaci√≥n Matem√°tica

**Capacidad vs. Datos:**
```
CNN t√≠pico: ~1M par√°metros
Datos disponibles: 51 √ó 60,000 = 3,060,000 valores

Ratio: 0.33 par√°metros/dato ‚Üí RIESGO MODERADO

Con autoencoder:
Pre-training: 71 √ó 60,000 = 4,260,000 valores
Fine-tuning: Solo classification head (~150k par√°metros)

Ratio: 0.035 par√°metros/dato ‚Üí BAJO RIESGO

NOTA: Aunque hay 71 mediciones, solo 51 son aisladores √∫nicos.
      La validaci√≥n debe usar GroupKFold para evitar leakage.
```

### ¬øPor qu√© Weighted Loss? (Etapa 2)

#### Problema: Desbalance Severo (42:7:2)

**Sin weighted loss:**
```
Si modelo predice siempre N1:
Accuracy = 42/51 = 82.4%
Recall N2 = 0%
Recall N3 = 0% ‚Üê ¬°INACEPTABLE!
```

**Con weighted loss:**
```
Weight N3 = 8.5 (21√ó mayor que N1)
Weight N2 = 2.4 (6√ó mayor que N1)
Loss cuando falla N3 = 21√ó loss cuando falla N1
‚Üí Modelo forzado a aprender N2 y N3

Ratio 42:7:2 es CR√çTICO - uno de los desbalances m√°s severos en SHM
```

**Evidencia:**
- Estudio 2022: Weighted loss mejora recall de clase minoritaria de 45% a 82%
- Meta-an√°lisis SHM: 85-90% de estudios con desbalance usan weighted loss

### Fundamento Te√≥rico de Features Relacionales

Las caracter√≠sticas relacionales entre S2 (excitaci√≥n base) y S1 (respuesta estructural) tienen fundamento en la teor√≠a de din√°mica estructural:

**Funci√≥n de Transferencia H(œâ):**
```
H(œâ) = S1(œâ) / S2(œâ)
```

**Ecuaci√≥n fundamental (Chopra 2017):**
$$|H(\omega)| = \frac{1}{\sqrt{[1-\beta^2]^2 + [2\xi\beta]^2}}$$

**Significado f√≠sico del da√±o:**
- **Aislador sano**: Aten√∫a altas frecuencias (H < 1 para f > f_n)
- **Aislador da√±ado**: Alteraci√≥n de atenuaci√≥n por cambios en rigidez/amortiguamiento
  - **Rigidez ‚Üì** ‚Üí œâ_n ‚Üì ‚Üí Pico de H(œâ) se desplaza a la izquierda
  - **Amortiguamiento ‚Üì** ‚Üí Pico de H(œâ) aumenta

**Referencias:**
- Yu et al. (2018): Cambios en H(œâ) correlacionan con nivel de da√±o
- Kelly & Konstantinidis (2011): Transmissibility en rango 0.1-15 Hz

**Implementaci√≥n pr√°ctica:**
En lugar de calcular H(œâ) completa, usamos estad√≠sticos simples (ratios, deltas) que capturan la esencia de la funci√≥n de transferencia sin la complejidad de arquitecturas dual-stream. Estos 18 features relacionales pueden agregarse opcionalmente si el an√°lisis de clustering muestra que mejoran la separabilidad entre clases.

---

## COMPARACI√ìN CON ALTERNATIVAS

### Opci√≥n A: CNN 1D Directo (Baseline)

```python
# Arquitectura simple desde cero
Input (6, 60000) ‚Üí Conv1D layers ‚Üí FC ‚Üí Softmax
```

**Pros:**
- ‚úÖ Simple de implementar
- ‚úÖ R√°pido de entrenar

**Contras:**
- ‚ùå Solo usa 51 aisladores √∫nicos (no aprovecha las 71 mediciones en aprendizaje no supervisado)
- ‚ùå Alto riesgo de overfitting con N2 (7) y especialmente N3 (2 aisladores)
- ‚ùå No aprovecha f√≠sica del sistema

**Performance esperado:** 87-90%

---

### Opci√≥n B: Transfer Learning con ResNet50 + CWT

```python
# Convertir se√±ales a espectrogramas (CWT)
# Usar ResNet50 pre-entrenado en ImageNet
```

**Pros:**
- ‚úÖ Leverage de pre-training en millones de im√°genes
- ‚úÖ Performance potencialmente alto (96-98%)
- ‚úÖ Arquitectura probada

**Contras:**
- ‚ùå No aprovecha las 71 mediciones en fase de pre-training (solo usa las 51 etiquetadas)
- ‚ùå CWT genera "im√°genes artificiales" (menos interpretable)
- ‚ùå Dif√≠cil integrar H(œâ) f√≠sico
- ‚ùå M√°s lento de entrenar (ResNet50 es pesado)

**Performance esperado:** 95-98%

---

### Opci√≥n C: Nuestra Propuesta (Autoencoder + CNN)

```python
# Etapa 1: Autoencoder (71 mediciones)
# Etapa 2: CNN classifier (51 aisladores √∫nicos)
# Opci√≥n: Agregar 18 features relacionales H(œâ) pre-calculadas
```

**Pros:**
- ‚úÖ Usa todas las 71 mediciones para pre-training (m√°ximo aprovechamiento)
- ‚úÖ Reduce overfitting con pre-training no supervisado
- ‚úÖ Opci√≥n de incorporar features relacionales H(œâ) si clustering muestra que son √∫tiles
- ‚úÖ Alta interpretabilidad para tesis
- ‚úÖ Arquitectura simple y comprensible
- ‚úÖ Aprovecha 20 mediciones repetidas para mayor robustez del encoder

**Contras:**
- ‚ö†Ô∏è Requiere pre-entrenamiento del autoencoder
- ‚ö†Ô∏è M√°s tiempo de desarrollo que CNN directo

**Performance esperado:** 94-97%

---

### Comparativa Final

| Criterio | CNN Directo | ResNet50+CWT | **Nuestra Propuesta** |
|----------|-------------|--------------|----------------------|
| **Usa todas las mediciones** | ‚ùå (51 √∫nicos) | ‚ùå (51 √∫nicos) | ‚úÖ (71 mediciones) |
| **Reduce overfitting** | ‚ö†Ô∏è Media | ‚úÖ Alta | ‚úÖ Muy Alta |
| **Interpretabilidad** | ‚ö†Ô∏è Baja | ‚ö†Ô∏è Baja | ‚úÖ Alta |
| **Validaci√≥n f√≠sica** | ‚ùå No | ‚ùå No | ‚úÖ S√≠ (H(œâ)) |
| **Tiempo implementaci√≥n** | ‚úÖ R√°pido | ‚ö†Ô∏è Medio | ‚ö†Ô∏è Lento |
| **Performance esperado** | 87-90% | 95-98% | **94-97%** |
| **Contribuci√≥n tesis** | ‚ö†Ô∏è B√°sica | ‚ö†Ô∏è Media | ‚úÖ Alta |

**Recomendaci√≥n:** **Nuestra Propuesta** porque:
1. Maximiza uso de datos disponibles (71 mediciones vs 51 aisladores √∫nicos)
2. Reduce riesgo de overfitting (CR√çTICO con solo 7 N2 y 2 N3)
3. Incorpora conocimiento f√≠sico (diferenciador clave)
4. Alta interpretabilidad (importante para tesis y aplicaci√≥n pr√°ctica)
5. Aprovecha 20 mediciones repetidas para mayor robustez del encoder

**ADVERTENCIA**: El desbalance 42:7:2 es EXTREMO. Considerar seriamente clasificaci√≥n binaria (N1 vs Damaged) como alternativa m√°s robusta.

---

## CONCLUSIONES Y PR√ìXIMOS PASOS

### Resumen de la Propuesta

1. **Arquitectura en 2 etapas** que maximiza uso de datos limitados:
   - Etapa 1: Autoencoder aprovecha las 71 mediciones de 51 aisladores √∫nicos
   - Etapa 2: CNN clasificador con transfer learning, con opci√≥n de agregar features relacionales H(œâ) pre-calculadas

2. **Performance esperado:**
   - 94-97% accuracy (basado en benchmarks de literatura, PERO desbalance 42:7:2 es m√°s severo que casos reportados)
   - Recall N2 y N3 > 80% (CR√çTICO para detectar da√±o con solo 7 N2 y 2 N3)
   - Reducci√≥n de variabilidad vs. clasificaci√≥n manual por expertos

3. **Contribuciones originales:**
   - Primera aplicaci√≥n de autoencoder+CNN a aisladores s√≠smicos
   - Opci√≥n de incorporar features relacionales H(œâ) pre-calculadas
   - Metodolog√≠a para datasets peque√±os con desbalance EXTREMO (42:7:2)
   - Aprovechamiento de mediciones repetidas para robustez del encoder

4. **ADVERTENCIA IMPORTANTE:**
   - El ratio 42:7:2 (21:3.5:1) es uno de los m√°s severos en literatura SHM
   - Considerar clasificaci√≥n binaria (N1 vs Damaged: N2+N3) como alternativa m√°s robusta

### Pr√≥ximos Pasos Inmediatos

1. **Revisar y aprobar esta propuesta**
   - Discutir arquitectura y justificaciones
   - Identificar posibles ajustes o mejoras
   - Alinear con objetivos de la tesis

2. **Setup del proyecto**
   - Crear estructura de directorios
   - Instalar dependencias
   - Preparar datos en formato correcto

3. **Comenzar Fase 1: Exploraci√≥n**
   - An√°lisis exploratorio de las 71 mediciones (51 aisladores √∫nicos)
   - Validar calidad de datos y estandarizaci√≥n de longitudes (58,700 a 141,800 ‚Üí 60,000)
   - Identificar 20 mediciones repetidas y estrategia de uso
   - Visualizaciones preliminares de H(œâ) y an√°lisis de separabilidad entre clases
   - **DECISI√ìN CR√çTICA**: ¬øClasificaci√≥n 3-class (N1/N2/N3) o binaria (N1 vs Damaged)?

---

**¬øPreguntas? ¬øAjustes necesarios? ¬øListo para comenzar implementaci√≥n?**

---

*Documento generado: Enero 2026*
*√öltima actualizaci√≥n: 2026-01-28*
