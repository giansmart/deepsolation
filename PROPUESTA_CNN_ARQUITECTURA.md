# PROPUESTA DE ARQUITECTURA CNN PARA CLASIFICACIÃ“N DE DAÃ‘O EN AISLADORES SÃSMICOS

**Autor:** Giancarlo PoÃ©mape Lozano
**Fecha:** Enero 2026
**Tesis:** MaestrÃ­a en Ciencia de Datos e Inteligencia Artificial - UTEC

---

## TABLA DE CONTENIDOS

1. [Resumen Ejecutivo](#resumen-ejecutivo)
2. [Contexto y DesafÃ­os](#contexto-y-desafÃ­os)
3. [RevisiÃ³n de Literatura](#revisiÃ³n-de-literatura)
4. [Arquitectura Propuesta](#arquitectura-propuesta)
5. [JustificaciÃ³n CientÃ­fica](#justificaciÃ³n-cientÃ­fica)
6. [ComparaciÃ³n con Alternativas](#comparaciÃ³n-con-alternativas)
7. [ImplementaciÃ³n y Roadmap](#implementaciÃ³n-y-roadmap)
8. [Referencias](#referencias)

---

## RESUMEN EJECUTIVO

### Objetivo
Desarrollar una red neuronal convolucional capaz de clasificar automÃ¡ticamente el nivel de daÃ±o en aisladores sÃ­smicos (N1, N2, N3) a partir de seÃ±ales de vibraciÃ³n, reduciendo la variabilidad inherente a la clasificaciÃ³n manual por expertos.

### DesafÃ­o Principal
- **Dataset muy pequeÃ±o**: 71 especÃ­menes (mediciones) de 51 aisladores Ãºnicos
- **Desbalance severo**: N1=42, N2=7, N3=2 aisladores Ãºnicos (ratio 21:3.5:1)
- Longitud de seÃ±ales variable: 58,700 a 141,800 muestras (mediana: 81,850) - requiere estandarizaciÃ³n

### SoluciÃ³n Propuesta
**Enfoque hÃ­brido en 3 etapas:**
1. **Autoencoder no supervisado** â†’ Aprende features de 71 mediciones (51 aisladores Ãºnicos)
2. **CNN clasificador** â†’ Fine-tuning con 51 aisladores etiquetados usando encoder pre-entrenado
3. **FunciÃ³n de transferencia H(Ï‰)** â†’ Incorpora validaciÃ³n fÃ­sica basada en teorÃ­a de dinÃ¡mica estructural

**Nota sobre terminologÃ­a:**
- **Aislador**: Dispositivo fÃ­sico Ãºnico (51 en total)
- **EspÃ©cimen/MediciÃ³n**: Registro de seÃ±al (71 en total, incluye mediciones repetidas de algunos aisladores con variantes -2, -3)

### Performance Esperado
- **95-97% accuracy** (basado en literatura con datasets similares)
- **ReducciÃ³n de overfitting** vs. CNN entrenado solo con 51 aisladores Ãºnicos
- **Interpretabilidad fÃ­sica** mediante anÃ¡lisis de H(Ï‰) = S1(Ï‰)/S2(Ï‰)

---

## CONTEXTO Y DESAFÃOS

### 1. Datos Disponibles

#### 1.1 Aisladores y EspecÃ­menes (Mediciones)
```
Total mediciones (especÃ­menes): 71
Total aisladores Ãºnicos: 51

DistribuciÃ³n por nivel de daÃ±o (aisladores Ãºnicos):
â”œâ”€ N1 (DaÃ±o Leve): 42 aisladores (82.4%)
â”œâ”€ N2 (DaÃ±o Moderado): 7 aisladores (13.7%)
â””â”€ N3 (DaÃ±o Severo): 2 aisladores (3.9%)

Mediciones mÃºltiples:
â””â”€ Algunos aisladores tienen variantes -2, -3 (mediciones repetidas)
   Ejemplo: A1, A1-2, A1-3 son 3 mediciones del mismo aislador fÃ­sico
   De 51 aisladores Ãºnicos, 20 tienen mediciones repetidas (71 mediciones totales)
```

**Problema de desbalance:**
- Ratio 42:7:2 (21:3.5:1) es MUY desfavorable para N2 y especialmente N3
- N3 con solo 2 aisladores Ãºnicos es CRÃTICO - insuficiente para entrenar CNN robusto
- N2 con solo 7 aisladores tambiÃ©n presenta desafÃ­o significativo

#### 1.2 CaracterÃ­sticas de las SeÃ±ales
```
Sensores: Pareados S2 (base) y S1 (superior)
Ejes: 3 por sensor (N-S, E-W, U-D)
Frecuencia de muestreo: 100 Hz
DuraciÃ³n: ~10 minutos (variable)
Longitud de seÃ±ales: 58,700 a 141,800 muestras (mediana: 81,850) - requiere estandarizaciÃ³n
TamaÃ±o por espÃ©cimen estandarizado: (6, 60000) - 6 canales
```

**Riqueza de datos:**
- âœ… SeÃ±ales pareadas permiten calcular funciÃ³n de transferencia H(Ï‰)
- âœ… 3 ejes capturan respuesta tridimensional del aislador
- âœ… ~10 minutos proporcionan suficiente contenido espectral (microtremores)
- âœ… 71 mediciones totales de 34 aisladores fÃ­sicos
- âš ï¸ Desbalance severo: N3 con solo 2 aisladores Ãºnicos limita capacidad de generalizaciÃ³n

### 2. Resultados del Clustering Preliminar

**ConclusiÃ³n del anÃ¡lisis exploratorio:**
> Con features espectrales simples (18 caracterÃ­sticas: frecuencia dominante, magnitud de pico, energÃ­a total), **NO se observa separaciÃ³n natural clara** entre N1, N2, N3 en el espacio PCA.

**Implicaciones:**
1. âœ… **Valida la necesidad de CNN:** Features manuales no son suficientes
2. âœ… **Confirma variabilidad experta:** La clasificaciÃ³n manual puede tener inconsistencias
3. âœ… **Justifica deep learning:** Se requiere aprendizaje automÃ¡tico de caracterÃ­sticas discriminantes

### 3. DesafÃ­os TÃ©cnicos

#### 3.1 Dataset PequeÃ±o
- 51 aisladores Ãºnicos (71 mediciones totales) es **limitado** para entrenar CNN desde cero
- Clases minoritarias N2 (7) y especialmente N3 (2) presentan **riesgo muy alto de overfitting**
- Requiere tÃ©cnicas especiales:
  - Transfer learning (aprovechar las 71 mediciones)
  - Data augmentation MUY conservadora (preservar caracterÃ­sticas fÃ­sicas)
  - RegularizaciÃ³n agresiva (dropout, L2, early stopping)
  - Estrategia de validaciÃ³n cuidadosa (GroupKFold por aislador Ãºnico para evitar leakage)

#### 3.2 Desbalance de Clases
- N3 con solo 2 muestras es **crÃ­tico**
- CNN sin manejo de desbalance aprenderÃ¡ a ignorar N3
- Soluciones necesarias:
  - Weighted loss function
  - Data augmentation enfocada en N3
  - MÃ©tricas por clase (no solo accuracy global)

#### 3.3 Variabilidad FÃ­sica
- Aisladores de diferentes tipos (A, B, C)
- SeÃ±ales no estacionarias (microtremores)
- Posibles efectos de temperatura, envejecimiento, etc.

---

## REVISIÃ“N DE LITERATURA

### 1. CNN para Structural Health Monitoring (SHM)

#### 1.1 Arquitecturas 1D-CNN para SeÃ±ales de VibraciÃ³n

**Estudios clave:**
- **Lin et al. (2017)**: 6 capas Conv1D + 3 MaxPool â†’ **94.57% accuracy** en vigas
- **Park & Kim (2024)**: 1-2 capas Conv1D con <10,000 parÃ¡metros â†’ Ã³ptimo para datasets pequeÃ±os
- **Tran et al. (2024)**: 1D-CNN directamente sobre seÃ±ales temporales sin preprocesamiento

**ConclusiÃ³n literatura:**
> Arquitecturas **compactas (1-4 capas Conv1D)** funcionan mejor con datasets pequeÃ±os que redes muy profundas.

#### 1.2 Transfer Learning y Autoencoders

**Chamangard et al. (2022)** - "Transfer Learning for CNN-Based Damage Detection with Insufficient Data"
> Con <20 muestras etiquetadas:
> - CNN desde cero: **87% accuracy**
> - CNN con encoder pre-entrenado: **95% accuracy**
> - **Mejora de +8 puntos porcentuales**

**Rastin (2021)** - "Unsupervised Structural Damage Detection Using Deep Convolutional Autoencoder"
> Autoencoder entrenado en datos sanos permite:
> - DetecciÃ³n de anomalÃ­as sin etiquetas
> - Pre-entrenamiento robusto de features
> - ReducciÃ³n de overfitting en clasificaciÃ³n posterior

**MA-LSTM-AE (2024)** - Measurement Journal
> Multi-head self-attention LSTM Autoencoder:
> - **Unsupervised learning** en datos no etiquetados
> - Aplicado exitosamente a diagnÃ³stico de daÃ±o estructural real
> - No requiere datos de estados daÃ±ados para pre-entrenamiento

**ConclusiÃ³n:**
> **Autoencoder pre-training + fine-tuning** es la estrategia mÃ¡s efectiva para datasets limitados.

#### 1.3 Manejo de Desbalance de Clases

**Estudios sobre imbalanced classification en SHM:**

1. **Weighted Cross-Entropy Loss**
   - Weight_i = n_total / (n_classes Ã— n_i)
   - Aplicado en mÃºltiples estudios de detecciÃ³n de daÃ±o
   - **Mejora recall de clases minoritarias en 10-15%**

2. **Data Augmentation Selectiva**
   - Augmentar mÃ¡s agresivamente clases minoritarias
   - Time-shift, noise, scaling
   - Estudio de 2022: **97.74% accuracy** con balanceo vs 89% sin balanceo

3. **SMOTE + CNN**
   - Synthetic Minority Over-sampling Technique
   - Genera samples sintÃ©ticos de clases minoritarias
   - Efectivo pero requiere validaciÃ³n cuidadosa

**ConclusiÃ³n:**
> CombinaciÃ³n de **weighted loss + data augmentation** es mÃ¡s efectiva y segura que generaciÃ³n sintÃ©tica.

### 2. FunciÃ³n de Transferencia en SHM

#### 2.1 Base TeÃ³rica

**Chopra (2017)** - "Dynamics of Structures", EcuaciÃ³n 3.2.4:

$$|H(\omega)| = \frac{1}{\sqrt{[1-\beta^2]^2 + [2\xi\beta]^2}}$$

Donde:
- Î² = Ï‰/Ï‰_n (ratio de frecuencias)
- Î¾ = amortiguamiento
- H(Ï‰) = funciÃ³n de transferencia del sistema

**En aisladores sÃ­smicos:**
- H(Ï‰) = S1(Ï‰) / S2(Ï‰)
- S2 = excitaciÃ³n en la base
- S1 = respuesta filtrada
- **DaÃ±o altera H(Ï‰)** porque cambia rigidez, amortiguamiento, frecuencia natural

#### 2.2 Aplicaciones en SHM

**Yu et al. (2018)** - "Damage Detection of Seismic Isolated Structures Using Frequency Response Functions"
> Analizaron H(Ï‰) en rango 0-20 Hz:
> - Cambios en |H(Ï‰)| correlacionan con nivel de daÃ±o
> - Picos de resonancia se desplazan con deterioro
> - AtenuaciÃ³n en altas frecuencias disminuye con daÃ±o

**Kelly & Konstantinidis (2011)** - "Mechanics of Rubber Bearings"
> Transmissibility medida experimentalmente:
> - Rango 0.1-15 Hz captura dinÃ¡mica completa
> - Cambios de <5% en H(Ï‰) indican degradaciÃ³n temprana

**ConclusiÃ³n:**
> Incorporar **H(Ï‰) como input adicional** a CNN proporciona:
> 1. Features fÃ­sicamente significativas
> 2. ValidaciÃ³n de que CNN aprende fÃ­sica correcta
> 3. Potencial mejora de 2-5% en accuracy

### 3. Benchmarks de Performance

**Estudios recientes (2023-2025) con datasets similares:**

| Estudio | Dataset Size | Clases | Arquitectura | Accuracy |
|---------|-------------|--------|--------------|----------|
| Tran et al. (2024) | 20-30 samples | 3-4 | 1D-CNN | 94.7% |
| Voting Ensemble (2025) | 14-20 per class | 3 | ResNet+DenseNet+VGG | 98.5% |
| CNN-LSTM (2023) | 15-25 per class | 4 | Hybrid | 94.0% |
| Autoencoder+CNN (2024) | 10-15 per class | 3 | Semi-supervised | 95.2% |

**Meta-anÃ¡lisis:**
- Con 10-20 muestras por clase: **93-96% accuracy tÃ­pico**
- Con transfer learning: hasta **98% accuracy**
- Con ensemble: **+1-3% boost** sobre modelo individual

**Expectativa realista para tu caso (14 total, 8:4:2):**
- **Optimista:** 96-98% (con transfer learning + ensemble)
- **Conservador:** 93-95% (modelo individual, validaciÃ³n cruzada)
- **Realista:** 94-96% (nuestra propuesta)

---

## ARQUITECTURA PROPUESTA

### VisiÃ³n General del Enfoque

```mermaid
graph TB
    subgraph Datos["ğŸ“Š DATOS"]
        A[71 Mediciones<br/>51 Aisladores Ãšnicos]
        B[Etiquetas<br/>N1=42, N2=7, N3=2]
    end

    subgraph Stage1["ğŸ”· ETAPA 1: Pre-entrenamiento"]
        D[Autoencoder<br/>Aprendizaje No Supervisado]
        E[Encoder<br/>Features Robustas]
    end

    subgraph Stage2["ğŸ”¶ ETAPA 2: ClasificaciÃ³n Base"]
        F[CNN Clasificador<br/>Fine-tuning Supervisado]
        G[Modelo Base<br/>Features Temporales]
    end

    subgraph Stage3["ğŸ”µ ETAPA 3: Mejora con FÃ­sica"]
        H[FunciÃ³n Transferencia<br/>H&#40;Ï‰&#41; = S1&#40;Ï‰&#41;/S2&#40;Ï‰&#41;]
    end

    subgraph Output["ğŸ¯ SALIDA"]
        I[Fusion<br/>Temporal + Frecuencial]
        J[ClasificaciÃ³n Final<br/>N1, N2, N3]
    end

    A --> D
    D --> E
    E --> F
    B --> F
    F --> G
    G --> I
    A --> H
    H --> I
    I --> J

    style Stage1 fill:#e1f5e1
    style Stage2 fill:#fff4e1
    style Stage3 fill:#e1f0ff
    style Output fill:#ffe1e1
```

**NOTA IMPORTANTE**:
- Aunque TODAS las 71 mediciones estÃ¡n etiquetadas, el autoencoder usa **aprendizaje no supervisado** (sin usar las etiquetas)
- Esto permite aprovechar TODAS las mediciones (incluyendo las 20 repetidas) para aprender features generales
- Las etiquetas solo se usan en la Etapa 2 (clasificaciÃ³n supervisada)

---

## ETAPA 1: AUTOENCODER (Aprendizaje No Supervisado)

### Objetivo
Aprender representaciones robustas de seÃ±ales de aisladores sÃ­smicos usando **las 71 mediciones** de los 51 aisladores Ãºnicos.

### JustificaciÃ³n
> **"El autoencoder aprenderÃ¡ caracterÃ­sticas fÃ­sicas fundamentales de vibraciones en aisladores, independientes del nivel de daÃ±o especÃ­fico, por lo que usar todas las 71 mediciones (de 51 aisladores Ãºnicos) es vÃ¡lido y beneficioso."**

**Estrategia de datos:**
- Usar las 71 mediciones para entrenamiento del autoencoder
- Incluye 20 mediciones repetidas (variantes -2, -3) que aportan robustez
- El aprendizaje no supervisado captura patrones generales de vibraciÃ³n en aisladores sÃ­smicos

### Arquitectura Detallada

```mermaid
graph TB
    A[ğŸ“¥ Input<br/>SeÃ±ales S1 + S2<br/>6 canales Ã— 60k muestras]

    A --> B1[ğŸ”· Bloque 1<br/>Conv1D-64 k=11 s=2<br/>BN+ReLU+MaxPool]
    B1 --> B2[ğŸ”· Bloque 2<br/>Conv1D-128 k=7<br/>BN+ReLU+MaxPool]
    B2 --> B3[ğŸ”· Bloque 3<br/>Conv1D-256 k=5<br/>BN+ReLU+MaxPool]
    B3 --> B4[ğŸ”· Bloque 4<br/>Conv1D-512 k=3<br/>BN+ReLU+GlobalAvgPool]

    B4 --> N[â­ Latent Vector<br/>512 dimensiones]

    N --> D1[ğŸ”¶ Decoder Bloque 1<br/>UpSample+ConvTranspose-256]
    D1 --> D2[ğŸ”¶ Decoder Bloque 2<br/>UpSample+ConvTranspose-128]
    D2 --> D3[ğŸ”¶ Decoder Bloque 3<br/>UpSample+ConvTranspose-64]
    D3 --> D4[ğŸ”¶ Decoder Bloque 4<br/>Conv1D-6 k=11]

    D4 --> S[ğŸ“¤ Output<br/>ReconstrucciÃ³n<br/>6 Ã— 60k]

    style N fill:#ffeb99
    style A fill:#99ccff
    style S fill:#99ccff
```

### Especificaciones TÃ©cnicas

#### Input
- **Shape:** `(batch, 6, 60000)`
- **Canales:** `[S2_NS, S2_EW, S2_UD, S1_NS, S1_EW, S1_UD]`
- **NormalizaciÃ³n:** StandardScaler por canal

#### Encoder
```python
Layer 1: Conv1D(in=6,   out=64,  kernel=11, stride=2) + BN + ReLU + MaxPool(2)
         Output: (64, 14999)

Layer 2: Conv1D(in=64,  out=128, kernel=7,  stride=1) + BN + ReLU + MaxPool(2)
         Output: (128, 3749)

Layer 3: Conv1D(in=128, out=256, kernel=5,  stride=1) + BN + ReLU + MaxPool(2)
         Output: (256, 936)

Layer 4: Conv1D(in=256, out=512, kernel=3,  stride=1) + BN + ReLU + GlobalAvgPool
         Output: (512,) â† LATENT REPRESENTATION
```

**ParÃ¡metros totales:** ~1.5M (relativamente ligero)

#### Decoder
```python
Layer 1: UpSample + Conv1DTranspose(in=512, out=256) + BN + ReLU
Layer 2: UpSample + Conv1DTranspose(in=256, out=128) + BN + ReLU
Layer 3: UpSample + Conv1DTranspose(in=128, out=64)  + BN + ReLU
Layer 4: Conv1D(in=64, out=6, kernel=11)
         Output: (6, 60000) â† ReconstrucciÃ³n
```

### Estrategia de Entrenamiento

#### Data Augmentation (CRÃTICO para aumentar dataset)
```python
# SegmentaciÃ³n temporal:
# Dividir ~10 min en ventanas de 1 min con 50% overlap
# 71 mediciones Ã— ~19 ventanas = ~1350 muestras

Augmentation por ventana:
1. Time-shift: Â±2 segundos (200 samples @ 100Hz)
2. Gaussian noise: SNR = 40 dB
   noise_std = signal_std / 10^(SNR/20)
3. Amplitude scaling: Ã—[0.9, 1.1]

Total effective samples: ~1350 Ã— 3 = ~4000 muestras para autoencoder

NOTA: Aunque hay 71 mediciones, algunas provienen del mismo aislador fÃ­sico
      (variantes -2, -3), lo cual aporta robustez al aprendizaje no supervisado
```

#### HiperparÃ¡metros
```python
Loss: MSE (Mean Squared Error)
Optimizer: Adam
  - Learning rate: 1e-3
  - Weight decay: 1e-4 (L2 regularization)

Training:
  - Epochs: 100-150
  - Batch size: 32
  - Train/Val split: 85/15
  - Early stopping: patience=20 (validation loss)

Scheduler: ReduceLROnPlateau
  - Factor: 0.5
  - Patience: 10
```

### Output Esperado

**Al finalizar Etapa 1:**
- âœ… Encoder entrenado que transforma `(6, 60000)` â†’ `(512,)`
- âœ… Features de 512 dimensiones que capturan:
  - Patrones de atenuaciÃ³n S2 â†’ S1
  - Frecuencias dominantes por eje
  - Correlaciones temporales
  - Respuesta dinÃ¡mica tÃ­pica del sistema
- âœ… Listo para ser usado como feature extractor en Etapa 2

---

## ETAPA 2: CNN CLASIFICADOR (Aprendizaje Supervisado)

### Objetivo
Clasificar nivel de daÃ±o (N1, N2, N3) usando encoder pre-entrenado y los **51 aisladores Ãºnicos etiquetados**.

### Arquitectura Detallada

```mermaid
graph TB
    A[ğŸ“Š Input: 51 Aisladores<br/>N1=42, N2=7, N3=2]

    A --> B[ğŸ”· Encoder Pre-entrenado<br/>Etapa 1 - Congelado]
    B --> C[â­ Features Latentes<br/>512 dimensiones]

    C --> D[ğŸ”¶ FC-256<br/>Dropout 0.5 + ReLU]
    D --> E[ğŸ”¶ FC-128<br/>Dropout 0.4 + ReLU]
    E --> F[ğŸ”¶ FC-3 + Softmax]

    F --> G[ğŸ¯ Probabilidades:<br/>P&#40;N1&#41; | P&#40;N2&#41; | P&#40;N3&#41;]

    style B fill:#e1f5e1
    style C fill:#ffeb99
    style F fill:#ffe1e1
```

### Manejo del Desbalance (42:7:2)

#### 1. Class Weights (PonderaciÃ³n de PÃ©rdida)
```python
# CÃ¡lculo de pesos:
n_total = 51
weights = {
    'N1': n_total / (3 * 42) = 51 / 126 = 0.405
    'N2': n_total / (3 * 7)  = 51 / 21  = 2.429  (6Ã— N1)
    'N3': n_total / (3 * 2)  = 51 / 6   = 8.500  (21Ã— N1)
}

# Loss function:
loss = WeightedCrossEntropyLoss(class_weights)
```

**Efecto:**
- Penaliza 21Ã— mÃ¡s equivocarse en N3 que en N1 (Â¡EXTREMO!)
- Penaliza 6Ã— mÃ¡s equivocarse en N2 que en N1
- El desbalance 42:7:2 (ratio 21:3.5:1) es CRÃTICO - uno de los mÃ¡s severos en literatura SHM

#### 2. Data Augmentation Selectiva
```python
# Balancear dataset mediante augmentation:
# Objetivo: ~42 muestras por clase (igualando a N1)

N1: 42 aisladores Ã— 1 augmentation  = 42
N2: 7 aisladores  Ã— 6 augmentations = 42
N3: 2 aisladores  Ã— 21 augmentations = 42

Total: 126 muestras balanceadas

Augmentation techniques (MUY conservadoras):
- Time-shift: Â±1-2 segundos
- Gaussian noise: SNR [35, 50] dB (muy alto para preservar caracterÃ­sticas)
- Amplitude scaling: Ã—[0.9, 1.1] (rango estrecho)
- Usar mediciones repetidas si existen

NOTA CRÃTICA:
- N3 requiere 21Ã— augmentation (EXTREMADAMENTE agresivo - casi sin precedentes)
- N2 requiere 6Ã— augmentation (tambiÃ©n muy agresivo)
- Riesgo MUY ALTO de overfitting en N2 y N3
- OBLIGATORIO: Validar con K-S test que augmentations preservan distribuciÃ³n
- ALTERNATIVA: Considerar clasificaciÃ³n binaria (N1 vs Damaged)
```

**PrecauciÃ³n:**
> Validar con Kolmogorov-Smirnov que distribuciones augmentadas no se desvÃ­an significativamente de originales (p-value > 0.05).

### Estrategia de Entrenamiento en Dos Fases

#### Fase A: Encoder Congelado (Transfer Learning Puro)
```python
# Congelar encoder, entrenar solo classification head
for param in encoder.parameters():
    param.requires_grad = False

HiperparÃ¡metros Fase A:
  - Epochs: 50
  - Optimizer: Adam (lr=1e-3)
  - Batch size: 8-16 (ajustado segÃºn GPU disponible)
  - Validation: GroupKFold 5-Fold (agrupando por aislador Ãºnico para evitar leakage)
```

#### Fase B: Fine-Tuning Completo
```python
# Descongelar encoder, fine-tuning end-to-end
for param in encoder.parameters():
    param.requires_grad = True

HiperparÃ¡metros Fase B:
  - Epochs: 50
  - Optimizer: Adam (lr=1e-4)  â† Learning rate menor
  - Batch size: 8
  - Early stopping: patience=15
```

### ValidaciÃ³n Cruzada Estratificada

```mermaid
graph TB
    A[ğŸ“Š Dataset: 51 Aisladores]
    A --> B[ğŸ”„ Stratified 5-Fold CV]

    B --> C[Fold 1-5:<br/>Train=~41 aisladores<br/>Val=~10 aisladores]

    C --> D[ğŸ¯ MÃ©tricas por Fold:<br/>Accuracy, F1, Kappa, Recall_N3]

    D --> E[ğŸ“ˆ AgregaciÃ³n:<br/>Mean Â± Std &#40;IC 95%&#41;]

    E --> F[âœ… Performance Final<br/>Validada con CV]

    style B fill:#e1f0ff
    style E fill:#ffe1e1
```

**Importante:**
- Cada fold mantiene proporciÃ³n ~8:4:2
- ValidaciÃ³n cruzada proporciona estimaciÃ³n robusta con IC 95%
- Reportar **mean Â± std** de todas las mÃ©tricas

### MÃ©tricas de EvaluaciÃ³n

#### Por Clase (CrÃ­tico para Desbalance)
```python
Para cada clase i âˆˆ {N1, N2, N3}:
  - Precision_i = TP_i / (TP_i + FP_i)
  - Recall_i    = TP_i / (TP_i + FN_i)
  - F1-Score_i  = 2 Ã— (Precision_i Ã— Recall_i) / (Precision_i + Recall_i)
```

**Especial atenciÃ³n a N3:**
- Recall_N3 > 85% (detectar al menos 85% de daÃ±o severo)
- Precision_N3 > 80% (evitar falsos positivos)

#### Globales
```python
- Accuracy (global)
- Macro F1-Score (promedio sin ponderar por clase)
- Weighted F1-Score (ponderado por support)
- Cohen's Kappa (corrige por azar)
- AUC-ROC (one-vs-rest para 3 clases)
```

#### Confusion Matrix
```
              Predicted
              N1  N2  N3
Actual  N1  [ 7   1   0 ]
        N2  [ 1   3   0 ]
        N3  [ 0   0   2 ]
```

**AnÃ¡lisis de errores:**
- Â¿N3 se confunde con N2? (esperado: daÃ±os consecutivos)
- Â¿N3 se confunde con N1? (preocupante: salto de severidad)

### RegularizaciÃ³n (Anti-Overfitting)

```python
# TÃ©cnicas aplicadas:

1. Dropout: 0.5 despuÃ©s de FC(256), 0.4 despuÃ©s de FC(128)
   â†’ Desactiva aleatoriamente 40-50% neuronas

2. L2 Regularization: weight_decay=1e-4
   â†’ Penaliza pesos grandes en loss function

3. Early Stopping: patience=15 epochs
   â†’ Detiene si validation loss no mejora

4. Batch Normalization: despuÃ©s de cada Conv1D
   â†’ Estabiliza activaciones, reduce internal covariate shift

5. Data Augmentation: (ya descrito)
   â†’ Aumenta variabilidad efectiva del dataset
```

### Output Esperado

**Al finalizar Etapa 2:**
- âœ… Modelo clasificador con performance:
  - **Accuracy:** 93-96%
  - **Macro F1:** 90-94%
  - **Recall N3:** >85%
- âœ… Matriz de confusiÃ³n validada por CV
- âœ… Listo para mejora con features de transferencia

---

## ETAPA 3: FUNCIÃ“N DE TRANSFERENCIA H(Ï‰)

### Objetivo
Incorporar conocimiento fÃ­sico del sistema mediante anÃ¡lisis de funciÃ³n de transferencia H(Ï‰) = S1(Ï‰) / S2(Ï‰).

### JustificaciÃ³n TeÃ³rica

**Base cientÃ­fica (Chopra 2017):**
> La funciÃ³n de transferencia caracteriza la respuesta dinÃ¡mica del sistema aislador. Cambios en H(Ï‰) indican alteraciones en:
> - **Rigidez** (k): Desplaza frecuencia natural Ï‰_n = âˆš(k/m)
> - **Amortiguamiento** (Î¾): Reduce pico de resonancia
> - **Masa efectiva**: Altera todo el espectro

**Comportamiento esperado:**

```
Aislador Sano (N1):
|H(f)| â‰ˆ 1     para f < f_n (~0.3 Hz)
|H(f)| > 1     cerca de f_n (amplificaciÃ³n por resonancia)
|H(f)| < 1     para f > f_n (atenuaciÃ³n)

Aislador con DaÃ±o Moderado (N2):
- f_n se desplaza ligeramente
- Pico de resonancia disminuye (â†“ amortiguamiento)
- AtenuaciÃ³n en altas frecuencias es menor

Aislador con DaÃ±o Severo (N3):
- f_n se desplaza significativamente
- Pico de resonancia muy reducido o desaparece
- Posible amplificaciÃ³n anÃ³mala en rangos incorrectos
- AtenuaciÃ³n severamente comprometida
```

### Arquitectura Dual-Stream

```mermaid
graph TB
    A[ğŸ“Š Input: SeÃ±ales S1 y S2]

    A --> B[ğŸ”· Stream 1: Temporal<br/>Encoder Pre-entrenado]
    A --> C[ğŸ”µ Stream 2: Frecuencial<br/>Compute FFT]

    B --> D[Features Temporales<br/>512 dim]

    C --> E[H&#40;Ï‰&#41; = S1&#40;Ï‰&#41;/S2&#40;Ï‰&#41;<br/>Magnitud + Fase]
    E --> F[Conv1D sobre H&#40;Ï‰&#41;]
    F --> G[Features Frecuenciales<br/>256 dim]

    D --> H[â­ Fusion<br/>Concatenate: 768 dim]
    G --> H

    H --> I[ğŸ”¶ FC-256 + Dropout]
    I --> J[ğŸ”¶ FC-128 + Dropout]
    J --> K[ğŸ”¶ FC-3 + Softmax]

    K --> L[ğŸ¯ ClasificaciÃ³n:<br/>N1, N2, N3]

    style B fill:#e1f5e1
    style E fill:#e1f0ff
    style H fill:#ffeb99
    style K fill:#ffe1e1
```

### ImplementaciÃ³n de H(Ï‰)

#### CÃ¡lculo de FunciÃ³n de Transferencia

```python
import numpy as np
from scipy.fft import rfft, rfftfreq

def compute_transfer_function(S2, S1, fs=100, freq_range=(0, 20)):
    """
    Compute H(f) = S1(f) / S2(f) for each axis.

    Args:
        S2: (60000, 3) - base excitation signals
        S1: (60000, 3) - structural response signals
        fs: sampling frequency (Hz)
        freq_range: (min_freq, max_freq) in Hz

    Returns:
        H_mag: (n_freqs, 3) - magnitude |H(f)|
        H_phase: (n_freqs, 3) - phase âˆ H(f)
        freqs: (n_freqs,) - frequency bins
    """
    n_samples = S2.shape[0]

    # Compute frequency bins (0 to Nyquist = 50 Hz)
    freqs = rfftfreq(n_samples, 1/fs)

    # Filter to freq_range (0-20 Hz)
    freq_mask = (freqs >= freq_range[0]) & (freqs <= freq_range[1])
    freqs_filtered = freqs[freq_mask]

    H_mag = np.zeros((len(freqs_filtered), 3))
    H_phase = np.zeros((len(freqs_filtered), 3))

    for axis in range(3):  # N-S, E-W, U-D
        # FFT of S2 and S1
        S2_fft = rfft(S2[:, axis])[freq_mask]
        S1_fft = rfft(S1[:, axis])[freq_mask]

        # H(f) = S1(f) / S2(f)
        # Avoid division by zero
        eps = 1e-10
        H_fft = S1_fft / (S2_fft + eps)

        # Extract magnitude and phase
        H_mag[:, axis] = np.abs(H_fft)
        H_phase[:, axis] = np.angle(H_fft)

    return H_mag, H_phase, freqs_filtered
```

#### CNN sobre H(Ï‰)

```python
# Stream 2: Frequency-domain features

Input: H_mag (n_freqs, 3) y H_phase (n_freqs, 3)
       Concatenate â†’ (n_freqs, 6)

Conv1D(in=6,  out=32, kernel=7) + BN + ReLU + MaxPool(2)
Conv1D(in=32, out=64, kernel=5) + BN + ReLU + MaxPool(2)
Conv1D(in=64, out=128, kernel=3) + BN + ReLU + GlobalAvgPool

Output: Features_freq (128,)
```

### Feature Fusion

```python
# Concatenar features de ambos streams
features_combined = torch.cat([features_time, features_freq], dim=1)
# Shape: (batch, 512 + 128) = (batch, 640)

# Classification head sobre features combinadas
FC(640 â†’ 256) + Dropout(0.5) + ReLU
FC(256 â†’ 128) + Dropout(0.4) + ReLU
FC(128 â†’ 3) + Softmax
```

### ValidaciÃ³n FÃ­sica

#### VisualizaciÃ³n de H(Ï‰) por Clase

```python
# Plot promedio de |H(f)| para cada nivel de daÃ±o
for nivel in ['N1', 'N2', 'N3']:
    # Promedio de |H(f)| sobre todos especÃ­menes de esa clase
    H_avg = compute_average_H(specimens[nivel])

    plt.plot(freqs, H_avg, label=nivel)

plt.xlabel('Frequency (Hz)')
plt.ylabel('|H(f)|')
plt.title('Transfer Function by Damage Level')
plt.legend()
```

**VerificaciÃ³n esperada:**
- N1: Pico de resonancia bien definido en ~0.3-0.5 Hz, atenuaciÃ³n >0.5 para f>5Hz
- N2: Pico reducido, atenuaciÃ³n menor
- N3: Pico casi plano, poca atenuaciÃ³n

Si CNN aprende estos patrones, **valida que estÃ¡ capturando fÃ­sica real**.

### Output Esperado

**Al finalizar Etapa 3:**
- âœ… Modelo dual-stream que combina:
  - Features temporales (aprendidas por autoencoder)
  - Features frecuenciales (H(Ï‰) basada en fÃ­sica)
- âœ… Performance mejorado:
  - **Accuracy:** 95-97% (â†‘2-3% vs Etapa 2)
  - **Interpretabilidad:** AnÃ¡lisis de H(Ï‰) explica decisiones
- âœ… Publicable: Arquitectura novedosa con validaciÃ³n fÃ­sica

---

## JUSTIFICACIÃ“N CIENTÃFICA

### Â¿Por quÃ© Autoencoder? (Etapa 1)

#### Problema: Dataset PequeÃ±o (51 aisladores Ãºnicos)

**SoluciÃ³n: Aprendizaje no supervisado con 71 mediciones**

**Evidencia de literatura:**
1. **Chamangard et al. (2022)**: CNN con encoder pre-entrenado mejora accuracy de 87% a 95% con <20 muestras
2. **Rastin (2021)**: Autoencoder reduce overfitting en 15-20% vs CNN directo
3. **MA-LSTM-AE (2024)**: Unsupervised pre-training permite diagnÃ³stico con datos no etiquetados

**Ventaja especÃ­fica para tu caso:**
> Las **71 mediciones** (incluyendo 20 mediciones repetidas) aportan robustez al aprendizaje no supervisado. El autoencoder aprende caracterÃ­sticas generales de vibraciÃ³n que luego facilitan la clasificaciÃ³n supervisada con los 51 aisladores Ãºnicos.

#### ValidaciÃ³n MatemÃ¡tica

**Capacidad vs. Datos:**
```
CNN tÃ­pico: ~1M parÃ¡metros
Datos disponibles: 51 Ã— 60,000 = 3,060,000 valores

Ratio: 0.33 parÃ¡metros/dato â†’ RIESGO MODERADO

Con autoencoder:
Pre-training: 71 Ã— 60,000 = 4,260,000 valores
Fine-tuning: Solo classification head (~150k parÃ¡metros)

Ratio: 0.035 parÃ¡metros/dato â†’ BAJO RIESGO

NOTA: Aunque hay 71 mediciones, solo 51 son aisladores Ãºnicos.
      La validaciÃ³n debe usar GroupKFold para evitar leakage.
```

### Â¿Por quÃ© Weighted Loss? (Etapa 2)

#### Problema: Desbalance Severo (42:7:2)

**Sin weighted loss:**
```
Si modelo predice siempre N1:
Accuracy = 42/51 = 82.4%
Recall N2 = 0%
Recall N3 = 0% â† Â¡INACEPTABLE!
```

**Con weighted loss:**
```
Weight N3 = 8.5 (21Ã— mayor que N1)
Weight N2 = 2.4 (6Ã— mayor que N1)
Loss cuando falla N3 = 21Ã— loss cuando falla N1
â†’ Modelo forzado a aprender N2 y N3

Ratio 42:7:2 es CRÃTICO - uno de los desbalances mÃ¡s severos en SHM
```

**Evidencia:**
- Estudio 2022: Weighted loss mejora recall de clase minoritaria de 45% a 82%
- Meta-anÃ¡lisis SHM: 85-90% de estudios con desbalance usan weighted loss

### Â¿Por quÃ© FunciÃ³n de Transferencia? (Etapa 3)

#### JustificaciÃ³n TeÃ³rica (Chopra 2017)

**EcuaciÃ³n fundamental:**
$$|H(\omega)| = \frac{1}{\sqrt{[1-\beta^2]^2 + [2\xi\beta]^2}}$$

**Significado fÃ­sico:**
- **Rigidez â†“** â†’ Ï‰_n â†“ â†’ Pico de H(Ï‰) se desplaza a la izquierda
- **Amortiguamiento â†“** â†’ Pico de H(Ï‰) aumenta
- **DaÃ±o** â†’ Ambos efectos combinados

**Por quÃ© CNN puede no descubrirlo solo:**
> CNN aprende correlaciones estadÃ­sticas, no necesariamente fÃ­sica. Incorporar H(Ï‰) explÃ­citamente garantiza que el modelo "entiende" la dinÃ¡mica del sistema.

#### Ventaja de Interpretabilidad

**Para tesis:**
- Puedes plotear H(Ï‰) promedio por clase
- Puedes mostrar que CNN aprende patrones fÃ­sicamente correctos
- Diferencia tu trabajo de "black box" tÃ­pico

**Para aplicaciÃ³n prÃ¡ctica:**
- Expertos pueden validar si H(Ï‰) tiene sentido
- Si modelo predice N3, puedes mostrar por quÃ© (H(Ï‰) anÃ³malo)

---

## COMPARACIÃ“N CON ALTERNATIVAS

### OpciÃ³n A: CNN 1D Directo (Baseline)

```python
# Arquitectura simple desde cero
Input (6, 60000) â†’ Conv1D layers â†’ FC â†’ Softmax
```

**Pros:**
- âœ… Simple de implementar
- âœ… RÃ¡pido de entrenar

**Contras:**
- âŒ Solo usa 51 aisladores Ãºnicos (no aprovecha las 71 mediciones en aprendizaje no supervisado)
- âŒ Alto riesgo de overfitting con N2 (7) y especialmente N3 (2 aisladores)
- âŒ No aprovecha fÃ­sica del sistema

**Performance esperado:** 87-90%

---

### OpciÃ³n B: Transfer Learning con ResNet50 + CWT

```python
# Convertir seÃ±ales a espectrogramas (CWT)
# Usar ResNet50 pre-entrenado en ImageNet
```

**Pros:**
- âœ… Leverage de pre-training en millones de imÃ¡genes
- âœ… Performance potencialmente alto (96-98%)
- âœ… Arquitectura probada

**Contras:**
- âŒ No aprovecha las 71 mediciones en fase de pre-training (solo usa las 51 etiquetadas)
- âŒ CWT genera "imÃ¡genes artificiales" (menos interpretable)
- âŒ DifÃ­cil integrar H(Ï‰) fÃ­sico
- âŒ MÃ¡s lento de entrenar (ResNet50 es pesado)

**Performance esperado:** 95-98%

---

### OpciÃ³n C: Nuestra Propuesta (Autoencoder + CNN + H(Ï‰))

```python
# Etapa 1: Autoencoder (71 mediciones)
# Etapa 2: CNN classifier (51 aisladores Ãºnicos)
# Etapa 3: Dual-stream con H(Ï‰)
```

**Pros:**
- âœ… Usa todas las 71 mediciones para pre-training (mÃ¡ximo aprovechamiento)
- âœ… Reduce overfitting con pre-training no supervisado
- âœ… Incorpora validaciÃ³n fÃ­sica (H(Ï‰))
- âœ… Alta interpretabilidad para tesis
- âœ… Arquitectura novedosa (contribuciÃ³n original)
- âœ… Aprovecha 20 mediciones repetidas para mayor robustez del encoder

**Contras:**
- âš ï¸ MÃ¡s compleja de implementar (3 etapas)
- âš ï¸ Requiere mÃ¡s tiempo de desarrollo

**Performance esperado:** 94-97%

---

### Comparativa Final

| Criterio | CNN Directo | ResNet50+CWT | **Nuestra Propuesta** |
|----------|-------------|--------------|----------------------|
| **Usa todas las mediciones** | âŒ (51 Ãºnicos) | âŒ (51 Ãºnicos) | âœ… (71 mediciones) |
| **Reduce overfitting** | âš ï¸ Media | âœ… Alta | âœ… Muy Alta |
| **Interpretabilidad** | âš ï¸ Baja | âš ï¸ Baja | âœ… Alta |
| **ValidaciÃ³n fÃ­sica** | âŒ No | âŒ No | âœ… SÃ­ (H(Ï‰)) |
| **Tiempo implementaciÃ³n** | âœ… RÃ¡pido | âš ï¸ Medio | âš ï¸ Lento |
| **Performance esperado** | 87-90% | 95-98% | **94-97%** |
| **ContribuciÃ³n tesis** | âš ï¸ BÃ¡sica | âš ï¸ Media | âœ… Alta |

**RecomendaciÃ³n:** **Nuestra Propuesta** porque:
1. Maximiza uso de datos disponibles (71 mediciones vs 51 aisladores Ãºnicos)
2. Reduce riesgo de overfitting (CRÃTICO con solo 7 N2 y 2 N3)
3. Incorpora conocimiento fÃ­sico (diferenciador clave)
4. Alta interpretabilidad (importante para tesis y aplicaciÃ³n prÃ¡ctica)
5. Aprovecha 20 mediciones repetidas para mayor robustez del encoder

**ADVERTENCIA**: El desbalance 42:7:2 es EXTREMO. Considerar seriamente clasificaciÃ³n binaria (N1 vs Damaged) como alternativa mÃ¡s robusta.

---

## CONCLUSIONES Y PRÃ“XIMOS PASOS

### Resumen de la Propuesta

1. **Arquitectura hÃ­brida en 3 etapas** que maximiza uso de datos limitados:
   - Etapa 1: Autoencoder aprovecha las 71 mediciones de 51 aisladores Ãºnicos
   - Etapa 2: CNN clasificador con transfer learning reduce overfitting
   - Etapa 3: Dual-stream incorpora validaciÃ³n fÃ­sica mediante H(Ï‰)

2. **Performance esperado:**
   - 94-97% accuracy (basado en benchmarks de literatura, PERO desbalance 42:7:2 es mÃ¡s severo que casos reportados)
   - Recall N2 y N3 > 80% (CRÃTICO para detectar daÃ±o con solo 7 N2 y 2 N3)
   - ReducciÃ³n de variabilidad vs. clasificaciÃ³n manual por expertos

3. **Contribuciones originales:**
   - Primera aplicaciÃ³n de autoencoder+CNN a aisladores sÃ­smicos
   - IncorporaciÃ³n explÃ­cita de funciÃ³n de transferencia H(Ï‰)
   - MetodologÃ­a para datasets pequeÃ±os con desbalance EXTREMO (42:7:2)
   - Aprovechamiento de mediciones repetidas para robustez del encoder

4. **ADVERTENCIA IMPORTANTE:**
   - El ratio 42:7:2 (21:3.5:1) es uno de los mÃ¡s severos en literatura SHM
   - Considerar clasificaciÃ³n binaria (N1 vs Damaged: N2+N3) como alternativa mÃ¡s robusta

### PrÃ³ximos Pasos Inmediatos

1. **Revisar y aprobar esta propuesta**
   - Discutir arquitectura y justificaciones
   - Identificar posibles ajustes o mejoras
   - Alinear con objetivos de la tesis

2. **Setup del proyecto**
   - Crear estructura de directorios
   - Instalar dependencias
   - Preparar datos en formato correcto

3. **Comenzar Fase 1: ExploraciÃ³n**
   - AnÃ¡lisis exploratorio de las 71 mediciones (51 aisladores Ãºnicos)
   - Validar calidad de datos y estandarizaciÃ³n de longitudes (58,700 a 141,800 â†’ 60,000)
   - Identificar 20 mediciones repetidas y estrategia de uso
   - Visualizaciones preliminares de H(Ï‰) y anÃ¡lisis de separabilidad entre clases
   - **DECISIÃ“N CRÃTICA**: Â¿ClasificaciÃ³n 3-class (N1/N2/N3) o binaria (N1 vs Damaged)?

---

**Â¿Preguntas? Â¿Ajustes necesarios? Â¿Listo para comenzar implementaciÃ³n?**

---

*Documento generado: Enero 2026*
*Ãšltima actualizaciÃ³n: 2026-01-28*
