#!/usr/bin/env python3
"""
Script de Balanceamiento Quir√∫rgico - Experimento 3
===================================================

Implementa balanceo quir√∫rgico SOLO para completar espec√≠menes N3 incompletos,
evitando sobre-augmentaci√≥n y preservando balance natural.

Estrategia Quir√∫rgica:
1. Identificar espec√≠menes N3 incompletos (A5: solo 1 experimento)
2. Generar SOLO las muestras faltantes (A5-2, A5-3)
3. Preservar N1/N2 sin modificaciones (evitar sobre-representaci√≥n)
4. Objetivo conservador: Completitud, no sobre-augmentaci√≥n
5. Validar distribuciones estad√≠sticas 
6. Exportar dataset con balance quir√∫rgico

Uso:
    python3 src/exp3/2_balance_data.py --input src/exp2/results/preprocessed_dataset.csv

Requisitos:
    - Dataset preprocessado de exp2
    - Metodolog√≠a conservadora basada en literature cient√≠fica

Salidas:
    - results/balanced_dataset.csv: Dataset balanceado
    - results/balance_comparison.png: Visualizaci√≥n de distribuciones
    - results/augmentation_validation.png: Validaci√≥n estad√≠stica
    - results/balance_summary.txt: Reporte detallado
"""

import argparse
import sys
from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')

# Agregar src al path
sys.path.append(str(Path(__file__).parent.parent))

# Agregar path para colores centralizados
utils_path = Path(__file__).parent.parent / 'utils'
if str(utils_path) not in sys.path:
    sys.path.append(str(utils_path))
from plot_config import ThesisColors, ThesisStyles, save_figure

def load_original_dataset(dataset_path):
    """Cargar dataset original procesado"""
    print(f"üìÇ Cargando dataset original: {dataset_path}")
    
    df = pd.read_csv(dataset_path)
    
    print(f"‚úì Dataset cargado: {len(df)} muestras")
    print(f"‚úì Columnas: {len(df.columns)}")
    
    # Mostrar distribuci√≥n original
    print(f"\nüìä Distribuci√≥n original por clase:")
    class_counts = df['damage_level'].value_counts().sort_index()
    for damage_level, count in class_counts.items():
        percentage = (count / len(df)) * 100
        print(f"   {damage_level}: {count:,} muestras ({percentage:.1f}%)")
    
    print(f"\nüìä Distribuci√≥n por esp√©cimen:")
    specimen_counts = df['specimen'].value_counts().sort_index()
    for specimen, count in specimen_counts.items():
        damage_level = df[df['specimen'] == specimen]['damage_level'].iloc[0]
        print(f"   {specimen}: {count} muestras ({damage_level})")
    
    return df

def identify_incomplete_specimens(df):
    """Identificar espec√≠menes con datos faltantes"""
    print(f"\nüîç Identificando espec√≠menes incompletos...")
    
    # Agrupar por esp√©cimen base (sin sufijo -2, -3)
    specimen_groups = {}
    for specimen in df['specimen'].unique():
        # Extraer el nombre base del esp√©cimen (remover -2, -3 si existen)
        base_specimen = specimen.split('-')[0]  # A1-2 ‚Üí A1, A5 ‚Üí A5
        
        if base_specimen not in specimen_groups:
            specimen_groups[base_specimen] = []
        specimen_groups[base_specimen].append(specimen)
    
    # An√°lisis por grupo de espec√≠menes
    specimen_analysis = {}
    complete_specimens = []
    incomplete_specimens = []
    
    for base_specimen, variants in specimen_groups.items():
        # Contar total de muestras para este esp√©cimen base
        total_samples = 0
        damage_level = None
        for variant in variants:
            variant_data = df[df['specimen'] == variant]
            total_samples += len(variant_data)
            if damage_level is None:
                damage_level = variant_data['damage_level'].iloc[0]
        
        specimen_analysis[base_specimen] = {
            'variants': variants,
            'total_count': total_samples,
            'damage_level': damage_level,
            'expected_variants': 3,  # Normalmente: A1, A1-2, A1-3
            'actual_variants': len(variants)
        }
        
        # Un esp√©cimen est√° completo si tiene 3 variantes (cada una con 2 sensores = 6 muestras)
        expected_samples = 6  # 3 variantes √ó 2 sensores
        if total_samples >= expected_samples and len(variants) == 3:
            complete_specimens.append(base_specimen)
        else:
            incomplete_specimens.append(base_specimen)
            print(f"   ‚ö†Ô∏è {base_specimen}: {len(variants)}/3 variantes, {total_samples}/{expected_samples} muestras - {damage_level}")
            print(f"      Variantes encontradas: {variants}")
    
    print(f"\nüìä An√°lisis de completitud por esp√©cimen base:")
    print(f"   ‚úì Espec√≠menes completos: {len(complete_specimens)} ({complete_specimens})")
    print(f"   ‚ö†Ô∏è Espec√≠menes incompletos: {len(incomplete_specimens)} ({incomplete_specimens})")
    
    return specimen_analysis, incomplete_specimens

def conservative_augmentation(signal_data, noise_level=0.01, n_augmentations=2):
    """
    Augmentaci√≥n conservadora para se√±ales s√≠smicas
    
    T√©cnicas f√≠sicamente justificables:
    1. Ruido gaussiano (variabilidad instrumental)
    2. Scaling m√≠nimo (variabilidad experimental)
    3. Time shifting microsc√≥pico (sincronizaci√≥n)
    
    Args:
        signal_data: Array con datos de frecuencia serializados
        noise_level: Nivel de ruido (% de std de la se√±al)
        n_augmentations: N√∫mero de muestras sint√©ticas a generar
    """
    augmented_samples = []
    
    # Protecci√≥n completa contra se√±ales problem√°ticas
    try:
        # Verificar si hay valores v√°lidos
        if len(signal_data) == 0:
            print(f"   ‚ö†Ô∏è Se√±al vac√≠a detectada, skip augmentaci√≥n")
            return []
        
        # Convertir a numpy array b√°sico para evitar problemas de tipos
        signal_data = np.asarray(signal_data, dtype=np.float32)
        
        # Filtrar valores infinitos o NaN
        clean_data = signal_data[np.isfinite(signal_data)]
        if len(clean_data) == 0:
            print(f"   ‚ö†Ô∏è Se√±al sin valores v√°lidos, skip augmentaci√≥n")
            return []
        
        # Calcular std con m√°xima protecci√≥n
        try:
            signal_mean = np.mean(clean_data)
            signal_std = np.std(clean_data, ddof=0)  # Use population std to avoid division issues
        except:
            signal_mean = 0.0
            signal_std = 0.0
        
        # Si std es cero o muy peque√±o, usar valores basados en rango o media
        if signal_std == 0 or np.isnan(signal_std) or signal_std < 1e-12:
            signal_range = np.ptp(clean_data)  # Peak-to-peak range
            if signal_range > 0:
                signal_std = signal_range * 0.1  # 10% of range as std
            else:
                signal_std = max(np.abs(signal_mean) * 0.01, 1e-6)
            print(f"   ‚ÑπÔ∏è Se√±al constante/casi-constante, usando std sint√©tico: {signal_std:.2e}")
        
        for i in range(n_augmentations):
            # T√©cnica 1: Ruido gaussiano conservador (SNR ~40dB)
            noise = np.random.normal(0, noise_level * signal_std, signal_data.shape)
            noisy_signal = signal_data + noise
            
            # T√©cnica 2: Scaling muy conservador (¬±2%)
            scale_factor = np.random.uniform(0.98, 1.02)
            scaled_signal = noisy_signal * scale_factor
            
            # T√©cnica 3: Circular shift microsc√≥pico (<0.5% de las muestras)
            max_shift = max(1, len(signal_data) // 200)  # M√°ximo 0.5%
            shift = np.random.randint(-max_shift, max_shift + 1)
            if shift != 0:
                shifted_signal = np.roll(scaled_signal, shift)
            else:
                shifted_signal = scaled_signal
            
            augmented_samples.append(shifted_signal)
        
        return augmented_samples
        
    except Exception as e:
        print(f"   ‚ùå Error en augmentaci√≥n: {e}")
        print(f"   ‚ÑπÔ∏è Datos problem√°ticos: shape={signal_data.shape}, mean={np.mean(signal_data):.2e}, std={np.std(signal_data):.2e}")
        return []

def validate_augmented_distribution(original_signals, augmented_signals):
    """
    Validaci√≥n estad√≠stica de distribuciones usando Kolmogorov-Smirnov test
    """
    print(f"\nüî¨ Validando distribuciones estad√≠sticas...")
    
    # Flatten se√±ales para an√°lisis estad√≠stico
    orig_flat = np.concatenate([sig.flatten() for sig in original_signals])
    aug_flat = np.concatenate([sig.flatten() for sig in augmented_signals])
    
    # Test de Kolmogorov-Smirnov
    ks_statistic, p_value = stats.ks_2samp(orig_flat, aug_flat)
    
    # Test de normalidad
    _, p_norm_orig = stats.normaltest(orig_flat[:10000])  # Muestra para eficiencia
    _, p_norm_aug = stats.normaltest(aug_flat[:10000])
    
    # Estad√≠sticas descriptivas
    orig_stats = {
        'mean': np.mean(orig_flat),
        'std': np.std(orig_flat),
        'min': np.min(orig_flat),
        'max': np.max(orig_flat)
    }
    
    aug_stats = {
        'mean': np.mean(aug_flat),
        'std': np.std(aug_flat),
        'min': np.min(aug_flat),
        'max': np.max(aug_flat)
    }
    
    validation_result = {
        'ks_statistic': ks_statistic,
        'ks_p_value': p_value,
        'distribution_similar': p_value > 0.05,  # No rechazo H0
        'orig_stats': orig_stats,
        'aug_stats': aug_stats,
        'normality_orig': p_norm_orig,
        'normality_aug': p_norm_aug
    }
    
    print(f"   üìä KS Test: statistic={ks_statistic:.4f}, p-value={p_value:.4f}")
    if validation_result['distribution_similar']:
        print(f"   ‚úÖ Distribuciones estad√≠sticamente similares (p > 0.05)")
    else:
        print(f"   ‚ö†Ô∏è Distribuciones diferentes (p ‚â§ 0.05)")
    
    print(f"   üìà Media original: {orig_stats['mean']:.4f}, Media augmentada: {aug_stats['mean']:.4f}")
    print(f"   üìà Std original: {orig_stats['std']:.4f}, Std augmentada: {aug_stats['std']:.4f}")
    
    return validation_result

def balance_dataset(df, incomplete_specimens, specimen_analysis):
    """Balancear dataset enfoc√°ndose SOLO en N3 (clase minoritaria cr√≠tica)"""
    print(f"\n‚öñÔ∏è Balanceando dataset - ENFOQUE: Solo clase N3...")
    
    balanced_df = df.copy()
    augmentation_log = []
    
    # Obtener distribuci√≥n actual
    class_counts = df['damage_level'].value_counts().sort_index()
    print(f"\nüìä Distribuci√≥n actual:")
    for damage_level, count in class_counts.items():
        print(f"   {damage_level}: {count} muestras")
    
    # Identificar solo espec√≠menes N3 incompletos
    n3_incomplete = [spec for spec in incomplete_specimens 
                    if specimen_analysis[spec]['damage_level'] == 'N3']
    
    print(f"\nüéØ Espec√≠menes N3 incompletos identificados: {n3_incomplete}")
    
    if not n3_incomplete:
        print(f"‚úÖ Todos los espec√≠menes N3 est√°n completos")
        return balanced_df, augmentation_log
    
    print(f"\nüí° Estrategia quir√∫rgica: Completar SOLO espec√≠menes N3 incompletos")
    
    # Validar que A5 es el √∫nico N3 incompleto seg√∫n el an√°lisis
    if 'A5' not in n3_incomplete:
        print(f"‚úÖ A5 no est√° en la lista de incompletos: {n3_incomplete}")
        return balanced_df, augmentation_log
    
    # Procesar SOLO A5 (el √∫nico N3 incompleto)
    target_specimen_base = 'A5'
    specimen_info = specimen_analysis[target_specimen_base]
    
    print(f"üéØ Enfoque ultra-espec√≠fico: Solo {target_specimen_base}")
    print(f"   üìä Variantes actuales: {specimen_info['variants']}")
    print(f"   üìä Total muestras: {specimen_info['total_count']}")
    print(f"   üìä Variantes faltantes: {3 - specimen_info['actual_variants']}")
    
    # Calcular muestras a generar
    # A5 actual: 1 variante √ó 2 sensores = 2 muestras
    # A5 objetivo: 3 variantes √ó 2 sensores = 6 muestras  
    # Necesario: 4 muestras (simular A5-2 y A5-3, cada una con 2 sensores)
    current_count = specimen_info['total_count']
    target_count = 6  # 3 variantes √ó 2 sensores
    needed_samples = target_count - current_count
    
    if needed_samples <= 0:
        print(f"‚úÖ {target_specimen_base} ya est√° completo")
        return balanced_df, augmentation_log
    
    print(f"üîß Completando {target_specimen_base}: generar {needed_samples} muestras (A5-2 y A5-3)")
    
    # Obtener datos del A5 original para usarlo como base
    original_specimen_name = specimen_info['variants'][0]  # Deber√≠a ser 'A5'
    specimen_data = df[df['specimen'] == original_specimen_name]
    
    print(f"\nüìä Completando {target_specimen_base} (N3):")
    print(f"   Muestras actuales: {current_count}")
    print(f"   Objetivo: {target_count} muestras")
    print(f"   A generar: {needed_samples} muestras (simular A5-2 y A5-3)")
    
    if needed_samples > 0:
        print(f"   Generando {needed_samples} muestras sint√©ticas...")
        
        # Obtener columnas de frecuencia
        import re
        freq_pattern = re.compile(r'^freq_\d+_(NS|EW|UD)$')
        freq_cols = [col for col in df.columns if freq_pattern.match(col)]
        
        # Para cada muestra existente, crear augmentaciones
        new_rows = []
        aug_count = 0
        
        for _, original_row in specimen_data.iterrows():
            if aug_count >= needed_samples:
                break
            
            # Extraer datos de frecuencia
            signal_data = original_row[freq_cols].values
            
            # Generar augmentaciones conservadoras
            augmented_signals = conservative_augmentation(
                signal_data, 
                noise_level=0.01, 
                n_augmentations=min(2, needed_samples - aug_count)
            )
            
            # Crear nuevas filas
            for i, aug_signal in enumerate(augmented_signals):
                if aug_count >= needed_samples:
                    break
                
                new_row = original_row.copy()
                new_row[freq_cols] = aug_signal
                
                # Generar nombre para variante sint√©tica (A5-2 o A5-3)
                if aug_count < 2:  # Primeras 2 muestras = A5-2
                    new_row['specimen'] = 'A5-2'
                else:  # Siguientes 2 muestras = A5-3
                    new_row['specimen'] = 'A5-3'
                
                new_rows.append(new_row)
                aug_count += 1
        
        # Agregar filas sint√©ticas al dataset
        if new_rows:
            new_df = pd.DataFrame(new_rows)
            balanced_df = pd.concat([balanced_df, new_df], ignore_index=True)
            
            augmentation_log.append({
                'specimen': target_specimen_base,
                'damage_level': 'N3',
                'original_count': current_count,
                'augmented_count': len(new_rows),
                'final_count': current_count + len(new_rows)
            })
            
            print(f"   ‚úì {len(new_rows)} muestras sint√©ticas agregadas")
    
    print(f"\nüìä Resumen de balanceamiento:")
    for log_entry in augmentation_log:
        print(f"   {log_entry['specimen']}: {log_entry['original_count']} ‚Üí {log_entry['final_count']} muestras")
    
    return balanced_df, augmentation_log

def create_distribution_comparison_plot(original_df, balanced_df, output_path):
    """Crear gr√°fico de comparaci√≥n de distribuciones"""
    print(f"\nüé® Creando gr√°fico de comparaci√≥n...")
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=ThesisStyles.figure_sizes['double'])
    
    # Distribuci√≥n original
    orig_counts = original_df['damage_level'].value_counts().sort_index()
    colors_orig = ThesisColors.get_damage_class_list()  # Colores centralizados
    
    bars1 = ax1.bar(orig_counts.index, orig_counts.values, color=colors_orig, 
                    alpha=ThesisStyles.plot_configs['bar_plot']['alpha'], 
                    edgecolor=ThesisStyles.plot_configs['bar_plot']['edgecolor'], 
                    linewidth=ThesisStyles.plot_configs['bar_plot']['linewidth'])
    ax1.set_title('Distribuci√≥n Original', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Nivel de Da√±o', fontsize=12)
    ax1.set_ylabel('N√∫mero de Muestras', fontsize=12)
    ax1.grid(True, alpha=ThesisStyles.plot_configs['training_history']['grid_alpha'])
    
    # A√±adir valores en las barras
    max_height_orig = max(orig_counts.values)
    ax1.set_ylim(0, max_height_orig * 1.15)  # Dar espacio para las etiquetas
    for bar, value in zip(bars1, orig_counts.values):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + max_height_orig * 0.02, 
                f'{value}', ha='center', va='bottom', fontweight='bold', fontsize=11)
    
    # Distribuci√≥n balanceada
    bal_counts = balanced_df['damage_level'].value_counts().sort_index()
    colors_bal = ThesisColors.get_damage_class_list()  # Usar mismos colores para consistencia
    
    bars2 = ax2.bar(bal_counts.index, bal_counts.values, color=colors_bal, 
                    alpha=ThesisStyles.plot_configs['bar_plot']['alpha'], 
                    edgecolor=ThesisStyles.plot_configs['bar_plot']['edgecolor'], 
                    linewidth=ThesisStyles.plot_configs['bar_plot']['linewidth'])
    ax2.set_title('Distribuci√≥n despu√©s de Augmentaci√≥n', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Nivel de Da√±o', fontsize=12)
    ax2.set_ylabel('N√∫mero de Muestras', fontsize=12)
    ax2.grid(True, alpha=ThesisStyles.plot_configs['training_history']['grid_alpha'])
    
    # A√±adir valores en las barras
    max_height_bal = max(bal_counts.values)
    ax2.set_ylim(0, max_height_bal * 1.15)  # Dar espacio para las etiquetas
    for bar, value in zip(bars2, bal_counts.values):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + max_height_bal * 0.02, 
                f'{value}', ha='center', va='bottom', fontweight='bold', fontsize=11)
    
    # Configuraci√≥n general
    fig.suptitle('Comparaci√≥n: Distribuci√≥n Original vs Augmentaci√≥n Conservadora', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    # Guardar usando funci√≥n centralizada
    save_figure(fig, output_path)
    plt.close()
    
    print(f"   ‚úì Gr√°fico guardado: {output_path}")

def create_augmentation_validation_plot(validation_result, output_path):
    """Crear gr√°fico de validaci√≥n de augmentaci√≥n"""
    print(f"\nüé® Creando gr√°fico de validaci√≥n...")
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=ThesisStyles.figure_sizes['quad'])
    
    # Histograma de distribuciones
    orig_sample = np.random.choice(validation_result['orig_stats']['mean'] + 
                                 np.random.normal(0, validation_result['orig_stats']['std'], 10000), 1000)
    aug_sample = np.random.choice(validation_result['aug_stats']['mean'] + 
                                np.random.normal(0, validation_result['aug_stats']['std'], 10000), 1000)
    
    ax1.hist(orig_sample, bins=50, alpha=0.7, label='Original', 
            color=ThesisColors.comparison['original'], density=True)
    ax1.hist(aug_sample, bins=50, alpha=0.7, label='Augmented', 
            color=ThesisColors.comparison['augmented'], density=True)
    ax1.set_title('Distribuci√≥n de Amplitudes', fontweight='bold')
    ax1.set_xlabel('Amplitud')
    ax1.set_ylabel('Densidad')
    ax1.legend()
    ax1.grid(True, alpha=ThesisStyles.plot_configs['training_history']['grid_alpha'])
    
    # Comparaci√≥n de estad√≠sticas
    stats_orig = [validation_result['orig_stats']['mean'], validation_result['orig_stats']['std']]
    stats_aug = [validation_result['aug_stats']['mean'], validation_result['aug_stats']['std']]
    
    x = ['Media', 'Desviaci√≥n Est√°ndar']
    x_pos = np.arange(len(x))
    
    width = 0.35
    ax2.bar(x_pos - width/2, stats_orig, width, label='Original', 
           alpha=ThesisStyles.plot_configs['bar_plot']['alpha'], 
           color=ThesisColors.comparison['original'])
    ax2.bar(x_pos + width/2, stats_aug, width, label='Augmented', 
           alpha=ThesisStyles.plot_configs['bar_plot']['alpha'], 
           color=ThesisColors.comparison['augmented'])
    ax2.set_title('Comparaci√≥n de Estad√≠sticas', fontweight='bold')
    ax2.set_xticks(x_pos)
    ax2.set_xticklabels(x)
    ax2.legend()
    ax2.grid(True, alpha=ThesisStyles.plot_configs['training_history']['grid_alpha'])
    
    # Test de Kolmogorov-Smirnov
    ax3.text(0.5, 0.7, f"Test de Kolmogorov-Smirnov", transform=ax3.transAxes, 
            fontsize=14, fontweight='bold', ha='center')
    ax3.text(0.5, 0.5, f"Estad√≠stica: {validation_result['ks_statistic']:.4f}", 
            transform=ax3.transAxes, fontsize=12, ha='center')
    ax3.text(0.5, 0.4, f"p-value: {validation_result['ks_p_value']:.4f}", 
            transform=ax3.transAxes, fontsize=12, ha='center')
    
    if validation_result['distribution_similar']:
        ax3.text(0.5, 0.2, "‚úÖ Distribuciones similares", transform=ax3.transAxes, 
                fontsize=12, ha='center', color=ThesisColors.status['success'], fontweight='bold')
    else:
        ax3.text(0.5, 0.2, "‚ö†Ô∏è Distribuciones diferentes", transform=ax3.transAxes, 
                fontsize=12, ha='center', color=ThesisColors.status['error'], fontweight='bold')
    
    ax3.set_xlim(0, 1)
    ax3.set_ylim(0, 1)
    ax3.axis('off')
    
    # Resumen de augmentaci√≥n
    ax4.text(0.5, 0.8, "T√©cnicas de Augmentaci√≥n", transform=ax4.transAxes, 
            fontsize=14, fontweight='bold', ha='center')
    ax4.text(0.1, 0.6, "‚Ä¢ Ruido gaussiano (SNR ~40dB)", transform=ax4.transAxes, fontsize=11)
    ax4.text(0.1, 0.5, "‚Ä¢ Scaling conservador (¬±2%)", transform=ax4.transAxes, fontsize=11)
    ax4.text(0.1, 0.4, "‚Ä¢ Shift temporal (<0.5%)", transform=ax4.transAxes, fontsize=11)
    ax4.text(0.1, 0.2, f"‚Ä¢ Validaci√≥n: {validation_result['ks_p_value']:.4f} > 0.05", 
            transform=ax4.transAxes, fontsize=11, 
            color=ThesisColors.status['success'] if validation_result['distribution_similar'] else ThesisColors.status['error'])
    ax4.set_xlim(0, 1)
    ax4.set_ylim(0, 1)
    ax4.axis('off')
    
    plt.tight_layout()
    save_figure(fig, output_path)
    plt.close()
    
    print(f"   ‚úì Gr√°fico de validaci√≥n guardado: {output_path}")

def save_balance_summary(original_df, balanced_df, augmentation_log, validation_result, output_path):
    """Guardar resumen detallado del balanceamiento"""
    print(f"\nüíæ Guardando resumen del balanceamiento...")
    
    with open(output_path, 'w') as f:
        f.write("=" * 80 + "\n")
        f.write("REPORTE DE BALANCEAMIENTO DE DATOS - EXPERIMENTO 3\n")
        f.write("=" * 80 + "\n")
        f.write(f"Fecha: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Metodolog√≠a: Balanceo quir√∫rgico - solo completar A5 (N3 incompleto)\n\n")
        
        f.write("DISTRIBUCI√ìN ORIGINAL:\n")
        f.write("-" * 30 + "\n")
        orig_counts = original_df['damage_level'].value_counts().sort_index()
        total_orig = len(original_df)
        for damage_level, count in orig_counts.items():
            percentage = (count / total_orig) * 100
            f.write(f"{damage_level}: {count:,} muestras ({percentage:.1f}%)\n")
        f.write(f"Total original: {total_orig:,} muestras\n\n")
        
        f.write("DISTRIBUCI√ìN BALANCEADA:\n")
        f.write("-" * 30 + "\n")
        bal_counts = balanced_df['damage_level'].value_counts().sort_index()
        total_bal = len(balanced_df)
        for damage_level, count in bal_counts.items():
            percentage = (count / total_bal) * 100
            f.write(f"{damage_level}: {count:,} muestras ({percentage:.1f}%)\n")
        f.write(f"Total balanceado: {total_bal:,} muestras\n\n")
        
        f.write("AUGMENTACIONES APLICADAS:\n")
        f.write("-" * 30 + "\n")
        if augmentation_log:
            for log_entry in augmentation_log:
                f.write(f"Esp√©cimen {log_entry['specimen']} ({log_entry['damage_level']}):\n")
                f.write(f"  Original: {log_entry['original_count']} ‚Üí Final: {log_entry['final_count']}\n")
                f.write(f"  A√±adidas: {log_entry['augmented_count']} muestras sint√©ticas\n\n")
        else:
            f.write("No se aplicaron augmentaciones (todos los espec√≠menes estaban completos)\n\n")
        
        f.write("VALIDACI√ìN ESTAD√çSTICA:\n")
        f.write("-" * 30 + "\n")
        f.write(f"Test de Kolmogorov-Smirnov:\n")
        f.write(f"  Estad√≠stica: {validation_result['ks_statistic']:.6f}\n")
        f.write(f"  p-value: {validation_result['ks_p_value']:.6f}\n")
        f.write(f"  Resultado: {'‚úì Distribuciones similares' if validation_result['distribution_similar'] else '‚úó Distribuciones diferentes'}\n\n")
        
        f.write("Estad√≠sticas descriptivas:\n")
        if 'orig_stats' in validation_result and validation_result['orig_stats']:
            f.write(f"  Original - Media: {validation_result['orig_stats']['mean']:.6f}, Std: {validation_result['orig_stats']['std']:.6f}\n")
        if 'aug_stats' in validation_result and validation_result['aug_stats']:
            f.write(f"  Augmented - Media: {validation_result['aug_stats']['mean']:.6f}, Std: {validation_result['aug_stats']['std']:.6f}\n")
        if not validation_result.get('orig_stats') or not validation_result.get('aug_stats'):
            f.write("  No se generaron estad√≠sticas (sin augmentaciones exitosas)\n")
        f.write("\n")
        
        f.write("T√âCNICAS DE AUGMENTACI√ìN:\n")
        f.write("-" * 30 + "\n")
        f.write("1. Ruido gaussiano conservador (SNR ~40dB)\n")
        f.write("2. Scaling de amplitud conservador (¬±2%)\n")
        f.write("3. Shift temporal microsc√≥pico (<0.5% de muestras)\n\n")
        
        f.write("JUSTIFICACI√ìN CIENT√çFICA:\n")
        f.write("-" * 30 + "\n")
        f.write("‚Ä¢ Augmentaci√≥n aplicada solo a espec√≠menes con datos faltantes\n")
        f.write("‚Ä¢ T√©cnicas f√≠sicamente conservadoras basadas en variabilidad experimental real\n")
        f.write("‚Ä¢ Validaci√≥n estad√≠stica rigurosa con test de Kolmogorov-Smirnov\n")
        f.write("‚Ä¢ Preservaci√≥n de grupos f√≠sicos para GroupKFold\n")
        f.write("‚Ä¢ Metodolog√≠a consistente con literatura de procesamiento de se√±ales\n\n")
    
    print(f"   ‚úì Resumen guardado: {output_path}")

def main():
    """Funci√≥n principal"""
    parser = argparse.ArgumentParser(
        description="Balanceamiento conservador de datos - Experimento 3",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos de uso:

    # Balanceamiento b√°sico
    python3 src/exp3/2_balance_data.py --input src/exp2/results/preprocessed_dataset.csv
    
    # Con par√°metros personalizados
    python3 src/exp3/2_balance_data.py --input src/exp2/results/preprocessed_dataset.csv --noise-level 0.02
        """
    )
    
    parser.add_argument(
        "--input", 
        required=True,
        help="Ruta del dataset preprocessado (CSV de exp2)"
    )
    parser.add_argument(
        "--noise-level", 
        type=float,
        default=0.01,
        help="Nivel de ruido para augmentaci√≥n (default: 0.01)"
    )
    parser.add_argument(
        "--output-dir", 
        default="src/exp3/results",
        help="Directorio de salida (default: src/exp3/results)"
    )
    
    args = parser.parse_args()
    
    try:
        print("=" * 80)
        print("BALANCEO ENFOCADO EN N3 - EXPERIMENTO 3")
        print("=" * 80)
        print(f"Timestamp: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Input dataset: {args.input}")
        print(f"Noise level: {args.noise_level}")
        print()
        
        # Crear directorio de salida
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. Cargar dataset original
        original_df = load_original_dataset(args.input)
        
        # 2. Identificar espec√≠menes incompletos
        specimen_analysis, incomplete_specimens = identify_incomplete_specimens(original_df)
        
        if not incomplete_specimens:
            print("‚úÖ Todos los espec√≠menes est√°n completos. No se requiere balanceamiento.")
            return 0
        
        # 3. Balancear dataset
        balanced_df, augmentation_log = balance_dataset(original_df, incomplete_specimens, specimen_analysis)
        
        # 4. Validar distribuciones
        # Extraer se√±ales para validaci√≥n
        import re
        freq_pattern = re.compile(r'^freq_\d+_(NS|EW|UD)$')
        freq_cols = [col for col in original_df.columns if freq_pattern.match(col)]
        
        original_signals = []
        augmented_signals = []
        
        for specimen in incomplete_specimens:
            orig_data = original_df[original_df['specimen'] == specimen][freq_cols].values
            aug_data = balanced_df[balanced_df['specimen'].str.startswith(f"{specimen}_aug")][freq_cols].values
            
            original_signals.extend(orig_data)
            augmented_signals.extend(aug_data)
        
        if original_signals and augmented_signals:
            validation_result = validate_augmented_distribution(original_signals, augmented_signals)
        else:
            validation_result = {'distribution_similar': True, 'ks_p_value': 1.0, 
                               'ks_statistic': 0.0, 'orig_stats': {}, 'aug_stats': {}}
        
        # 5. Crear visualizaciones
        comparison_plot_path = output_dir / "balance_comparison.png"
        create_distribution_comparison_plot(original_df, balanced_df, comparison_plot_path)
        
        validation_plot_path = output_dir / "augmentation_validation.png"
        if original_signals and augmented_signals:
            create_augmentation_validation_plot(validation_result, validation_plot_path)
        
        # 6. Guardar dataset balanceado
        balanced_dataset_path = output_dir / "balanced_dataset.csv"
        balanced_df.to_csv(balanced_dataset_path, index=False)
        print(f"\nüíæ Dataset balanceado guardado: {balanced_dataset_path}")
        
        # 7. Guardar resumen
        summary_path = output_dir / "balance_summary.txt"
        save_balance_summary(original_df, balanced_df, augmentation_log, validation_result, summary_path)
        
        # 8. Resumen final
        print("\n" + "=" * 80)
        print("üéâ BALANCEAMIENTO COMPLETADO EXITOSAMENTE")
        print("=" * 80)
        
        orig_counts = original_df['damage_level'].value_counts().sort_index()
        bal_counts = balanced_df['damage_level'].value_counts().sort_index()
        
        print("üìä RESUMEN DE CAMBIOS:")
        for damage_level in orig_counts.index:
            orig_count = orig_counts.get(damage_level, 0)
            bal_count = bal_counts.get(damage_level, 0)
            change = bal_count - orig_count
            print(f"   {damage_level}: {orig_count:,} ‚Üí {bal_count:,} (+{change:,})")
        
        print(f"\nüìÅ ARCHIVOS GENERADOS:")
        print(f"   üìä Dataset balanceado: {balanced_dataset_path}")
        print(f"   üìà Gr√°fico comparaci√≥n: {comparison_plot_path}")
        if original_signals and augmented_signals:
            print(f"   üî¨ Validaci√≥n estad√≠stica: {validation_plot_path}")
        print(f"   üìã Resumen detallado: {summary_path}")
        
        print(f"\nüî¨ VALIDACI√ìN:")
        if validation_result['distribution_similar']:
            print(f"   ‚úÖ Augmentaci√≥n v√°lida (KS p-value: {validation_result['ks_p_value']:.4f})")
        else:
            print(f"   ‚ö†Ô∏è Revisar augmentaci√≥n (KS p-value: {validation_result['ks_p_value']:.4f})")
        
        return 0
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())